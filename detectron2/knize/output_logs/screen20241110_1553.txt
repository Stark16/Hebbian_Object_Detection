Script started on 2024-11-10 15:53:38-06:00 [TERM="screen" TTY="/dev/pts/79" COLUMNS="156" LINES="41"]
jknize@aiscalar:~/main/repo/CSC578$ exitCUDA_VISIBLE_DEVICES=2 python[16Pcd detectron2..[KpythonCUDA_VISIBLE_DEVICES=2 python ../python/coco_subset/training_script.py[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpython[K[1Pcd ..detectron2CUDA_VISIBLE_DEVICES=2 pythonexit[K[KCUDAY[K_VISIBLE_DEVICES-3[K[K=3[K2 python
Python 3.8.10 (default, Nov 22 2023, 10:22:35) 
[GCC 9.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import os
>>> os.getcwd()
'/home/jknize/main/repo/CSC578'
>>> osc[K.chdir('detectron2')
>>> from detectron2.data.datasets import register_coco_instances
>>> from detectron2.data import DatasetCatalog, MetadataCatalog
>>> register_coco_instances("coco_train_dog", {}, "../datasets/coco/annotations/dog_instances_train2017.json", "../datasets/coco/train2017_dog")
>>> register_coco_instances("coco_val_dog", {}, "../datasets/coco/annotations/dog_instances_val2017.json", "../datasets/coco/val2017_dog")
>>> my_dataset_metadata = MetadataCatalog.get("coco_train_dog")
>>> my_dataset_metadata.thing_classes = ["dog"]
>>> from detectron2.config import get_cfg
>>> from detectron2.engine import DefaultTrainer
>>> import torch
>>> cfg = get_cfg()
>>> cfg.merge_from_file("configs/COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml") #ImageNet pre-trained
>>> cfg.OUTPUT_DIR = "knize/output/dog"
>>> cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5
>>> cfg.DATASETS.TRAIN = ("coco_train_dog",)
>>> cfg.DATASETS.TEST = ("coco_val_dog",)
>>> cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1
>>> cfg.SOLVER.IMS_PER_BATCH = 8
>>> cfg.SOLVER.MAX_ITER = 1700 # about 3 epochs[C[1P[1P[1P[1P[1@8[1@5[1@5[1P[1@0[1@0[C[C[C[C[C[C[C[C[C[C[1P epochs1 epochs5 epochs
>>> cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128
>>> cfg.MODEL.DEVICE = 'cuda'
>>> trainer = DefaultTrainer(cfg)
[32m[11/10 15:56:58 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=2, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
    )
  )
)
[32m[11/10 15:56:58 d2.data.datasets.coco]: [0mLoaded 4385 images in COCO format from ../datasets/coco/annotations/dog_instances_train2017.json
[32m[11/10 15:56:58 d2.data.build]: [0mRemoved 0 images with no usable annotations. 4385 images left.
[32m[11/10 15:56:58 d2.data.build]: [0mDistribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    dog     | 5500         |
|            |              |[0m
[32m[11/10 15:56:58 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[11/10 15:56:58 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[11/10 15:56:58 d2.data.common]: [0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[32m[11/10 15:56:58 d2.data.common]: [0mSerializing 4385 elements to byte tensors and concatenating them all ...
[32m[11/10 15:56:58 d2.data.common]: [0mSerialized dataset takes 6.35 MiB
[32m[11/10 15:56:58 d2.data.build]: [0mMaking batched data loader with batch_size=8
[5m[31mWARNING[0m [32m[11/10 15:56:58 d2.solver.build]: [0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
>>> trainer.resume_or_load(resume=False)
[32m[11/10 15:57:07 d2.checkpoint.detection_checkpoint]: [0m[DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-101.pkl ...
[32m[11/10 15:57:07 d2.checkpoint.c2_model_loading]: [0mRenaming Caffe2 weights ......
[32m[11/10 15:57:07 d2.checkpoint.c2_model_loading]: [0mFollowing weights matched with submodule backbone.bottom_up - Total num: 105
Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.fpn_lateral2.{bias, weight}[0m
[34mbackbone.fpn_lateral3.{bias, weight}[0m
[34mbackbone.fpn_lateral4.{bias, weight}[0m
[34mbackbone.fpn_lateral5.{bias, weight}[0m
[34mbackbone.fpn_output2.{bias, weight}[0m
[34mbackbone.fpn_output3.{bias, weight}[0m
[34mbackbone.fpn_output4.{bias, weight}[0m
[34mbackbone.fpn_output5.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mroi_heads.box_head.fc1.{bias, weight}[0m
[34mroi_heads.box_head.fc2.{bias, weight}[0m
[34mroi_heads.box_predictor.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.cls_score.{bias, weight}[0m
The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
>>> trainer.model.to(cfg.MODEL.DEVICE)
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=2, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
    )
  )
)
>>> trainer.train()
[32m[11/10 15:57:16 d2.engine.train_loop]: [0mStarting training from iteration 0
/home/jknize/.local/lib/python3.8/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[32m[11/10 15:57:24 d2.utils.events]: [0m eta: 0:52:54  iter: 19  total_loss: 0.6879  loss_cls: 0.1048  loss_box_reg: 0.002973  loss_rpn_cls: 0.6083  loss_rpn_loc: 0.01726    time: 0.3725  last_time: 0.3548  data_time: 0.0374  last_data_time: 0.0057   lr: 0.00039962  max_mem: 11503M
No protocol specified
[32m[11/10 15:57:34 d2.utils.events]: [0m eta: 0:52:15  iter: 39  total_loss: 0.6044  loss_cls: 0.2877  loss_box_reg: 0.1608  loss_rpn_cls: 0.131  loss_rpn_loc: 0.01249    time: 0.3706  last_time: 0.3802  data_time: 0.0267  last_data_time: 0.0341   lr: 0.00079922  max_mem: 11503M
[32m[11/10 15:57:41 d2.utils.events]: [0m eta: 0:51:53  iter: 59  total_loss: 0.607  loss_cls: 0.2322  loss_box_reg: 0.2688  loss_rpn_cls: 0.06918  loss_rpn_loc: 0.01397    time: 0.3688  last_time: 0.3624  data_time: 0.0268  last_data_time: 0.0289   lr: 0.0011988  max_mem: 11505M
[32m[11/10 15:57:48 d2.utils.events]: [0m eta: 0:51:10  iter: 79  total_loss: 0.5133  loss_cls: 0.1807  loss_box_reg: 0.2379  loss_rpn_cls: 0.05398  loss_rpn_loc: 0.01433    time: 0.3663  last_time: 0.3439  data_time: 0.0252  last_data_time: 0.0030   lr: 0.0015984  max_mem: 11505M
[32m[11/10 15:57:56 d2.utils.events]: [0m eta: 0:51:12  iter: 99  total_loss: 0.6135  loss_cls: 0.2183  loss_box_reg: 0.3547  loss_rpn_cls: 0.03784  loss_rpn_loc: 0.01123    time: 0.3704  last_time: 0.3962  data_time: 0.0264  last_data_time: 0.0029   lr: 0.001998  max_mem: 11505M
[32m[11/10 15:58:04 d2.utils.events]: [0m eta: 0:51:48  iter: 119  total_loss: 0.6542  loss_cls: 0.2064  loss_box_reg: 0.3643  loss_rpn_cls: 0.03663  loss_rpn_loc: 0.01076    time: 0.3783  last_time: 0.3802  data_time: 0.0275  last_data_time: 0.0404   lr: 0.0023976  max_mem: 11505M
[32m[11/10 15:58:13 d2.utils.events]: [0m eta: 0:52:56  iter: 139  total_loss: 0.6426  loss_cls: 0.1804  loss_box_reg: 0.3695  loss_rpn_cls: 0.04109  loss_rpn_loc: 0.01513    time: 0.3836  last_time: 0.4294  data_time: 0.0264  last_data_time: 0.0235   lr: 0.0027972  max_mem: 11505M
[32m[11/10 15:58:21 d2.utils.events]: [0m eta: 0:53:06  iter: 159  total_loss: 0.5911  loss_cls: 0.1746  loss_box_reg: 0.3746  loss_rpn_cls: 0.03127  loss_rpn_loc: 0.01126    time: 0.3868  last_time: 0.4233  data_time: 0.0258  last_data_time: 0.0336   lr: 0.0031968  max_mem: 11505M
[32m[11/10 15:58:29 d2.utils.events]: [0m eta: 0:53:43  iter: 179  total_loss: 0.6277  loss_cls: 0.1741  loss_box_reg: 0.3854  loss_rpn_cls: 0.03001  loss_rpn_loc: 0.01115    time: 0.3903  last_time: 0.4074  data_time: 0.0283  last_data_time: 0.0382   lr: 0.0035964  max_mem: 11505M
[32m[11/10 15:58:38 d2.utils.events]: [0m eta: 0:54:03  iter: 199  total_loss: 0.6391  loss_cls: 0.1718  loss_box_reg: 0.3984  loss_rpn_cls: 0.02813  loss_rpn_loc: 0.0124    time: 0.3949  last_time: 0.3901  data_time: 0.0282  last_data_time: 0.0508   lr: 0.003996  max_mem: 11505M
[32m[11/10 15:58:47 d2.utils.events]: [0m eta: 0:54:24  iter: 219  total_loss: 0.5938  loss_cls: 0.188  loss_box_reg: 0.3913  loss_rpn_cls: 0.02486  loss_rpn_loc: 0.01226    time: 0.3978  last_time: 0.3436  data_time: 0.0298  last_data_time: 0.0073   lr: 0.0043956  max_mem: 11505M
[32m[11/10 15:58:55 d2.utils.events]: [0m eta: 0:54:32  iter: 239  total_loss: 0.6297  loss_cls: 0.2054  loss_box_reg: 0.3952  loss_rpn_cls: 0.02663  loss_rpn_loc: 0.0109    time: 0.3996  last_time: 0.4300  data_time: 0.0278  last_data_time: 0.0362   lr: 0.0047952  max_mem: 11505M
[32m[11/10 15:59:03 d2.utils.events]: [0m eta: 0:54:34  iter: 259  total_loss: 0.5747  loss_cls: 0.1639  loss_box_reg: 0.3754  loss_rpn_cls: 0.01879  loss_rpn_loc: 0.01001    time: 0.4007  last_time: 0.4537  data_time: 0.0278  last_data_time: 0.0399   lr: 0.0051948  max_mem: 11505M
[32m[11/10 15:59:12 d2.utils.events]: [0m eta: 0:54:42  iter: 279  total_loss: 0.546  loss_cls: 0.1451  loss_box_reg: 0.3584  loss_rpn_cls: 0.01858  loss_rpn_loc: 0.01002    time: 0.4018  last_time: 0.3833  data_time: 0.0268  last_data_time: 0.0107   lr: 0.0055944  max_mem: 11505M
[32m[11/10 15:59:20 d2.utils.events]: [0m eta: 0:54:45  iter: 299  total_loss: 0.5573  loss_cls: 0.1541  loss_box_reg: 0.358  loss_rpn_cls: 0.01932  loss_rpn_loc: 0.01233    time: 0.4026  last_time: 0.4056  data_time: 0.0265  last_data_time: 0.0394   lr: 0.005994  max_mem: 11505M
[32m[11/10 15:59:28 d2.utils.events]: [0m eta: 0:54:44  iter: 319  total_loss: 0.5843  loss_cls: 0.1646  loss_box_reg: 0.3802  loss_rpn_cls: 0.02102  loss_rpn_loc: 0.01411    time: 0.4028  last_time: 0.4355  data_time: 0.0247  last_data_time: 0.0349   lr: 0.0063936  max_mem: 11505M
[32m[11/10 15:59:36 d2.utils.events]: [0m eta: 0:54:38  iter: 339  total_loss: 0.4867  loss_cls: 0.1398  loss_box_reg: 0.314  loss_rpn_cls: 0.01185  loss_rpn_loc: 0.01147    time: 0.4030  last_time: 0.3954  data_time: 0.0293  last_data_time: 0.0369   lr: 0.0067932  max_mem: 11505M
[32m[11/10 15:59:44 d2.utils.events]: [0m eta: 0:54:38  iter: 359  total_loss: 0.5571  loss_cls: 0.1658  loss_box_reg: 0.3623  loss_rpn_cls: 0.01794  loss_rpn_loc: 0.01178    time: 0.4040  last_time: 0.4759  data_time: 0.0256  last_data_time: 0.0312   lr: 0.0071928  max_mem: 11505M
[32m[11/10 15:59:53 d2.utils.events]: [0m eta: 0:54:34  iter: 379  total_loss: 0.5378  loss_cls: 0.1484  loss_box_reg: 0.3525  loss_rpn_cls: 0.01723  loss_rpn_loc: 0.01256    time: 0.4042  last_time: 0.4142  data_time: 0.0260  last_data_time: 0.0109   lr: 0.0075924  max_mem: 11505M
[32m[11/10 16:00:01 d2.utils.events]: [0m eta: 0:54:29  iter: 399  total_loss: 0.5095  loss_cls: 0.1588  loss_box_reg: 0.3255  loss_rpn_cls: 0.01454  loss_rpn_loc: 0.01053    time: 0.4046  last_time: 0.4591  data_time: 0.0265  last_data_time: 0.0375   lr: 0.007992  max_mem: 11505M
[32m[11/10 16:00:09 d2.utils.events]: [0m eta: 0:54:26  iter: 419  total_loss: 0.5873  loss_cls: 0.1763  loss_box_reg: 0.3736  loss_rpn_cls: 0.01331  loss_rpn_loc: 0.01221    time: 0.4052  last_time: 0.4213  data_time: 0.0272  last_data_time: 0.0088   lr: 0.0083916  max_mem: 11505M
[32m[11/10 16:00:18 d2.utils.events]: [0m eta: 0:54:29  iter: 439  total_loss: 0.562  loss_cls: 0.1684  loss_box_reg: 0.3625  loss_rpn_cls: 0.01685  loss_rpn_loc: 0.01206    time: 0.4058  last_time: 0.4407  data_time: 0.0308  last_data_time: 0.0398   lr: 0.0087912  max_mem: 11505M
[32m[11/10 16:00:26 d2.utils.events]: [0m eta: 0:54:21  iter: 459  total_loss: 0.5724  loss_cls: 0.186  loss_box_reg: 0.3376  loss_rpn_cls: 0.01726  loss_rpn_loc: 0.01061    time: 0.4058  last_time: 0.3869  data_time: 0.0263  last_data_time: 0.0123   lr: 0.0091908  max_mem: 11505M
[32m[11/10 16:00:34 d2.utils.events]: [0m eta: 0:54:18  iter: 479  total_loss: 0.5337  loss_cls: 0.1545  loss_box_reg: 0.3428  loss_rpn_cls: 0.01303  loss_rpn_loc: 0.01356    time: 0.4062  last_time: 0.3935  data_time: 0.0275  last_data_time: 0.0119   lr: 0.0095904  max_mem: 11505M
[32m[11/10 16:00:42 d2.utils.events]: [0m eta: 0:54:09  iter: 499  total_loss: 0.4914  loss_cls: 0.1528  loss_box_reg: 0.306  loss_rpn_cls: 0.01795  loss_rpn_loc: 0.01151    time: 0.4064  last_time: 0.4937  data_time: 0.0298  last_data_time: 0.0372   lr: 0.00999  max_mem: 11505M
[32m[11/10 16:00:50 d2.utils.events]: [0m eta: 0:54:05  iter: 519  total_loss: 0.6038  loss_cls: 0.1744  loss_box_reg: 0.362  loss_rpn_cls: 0.01979  loss_rpn_loc: 0.01144    time: 0.4065  last_time: 0.3556  data_time: 0.0250  last_data_time: 0.0065   lr: 0.01039  max_mem: 11505M
[32m[11/10 16:00:59 d2.utils.events]: [0m eta: 0:54:04  iter: 539  total_loss: 0.6678  loss_cls: 0.18  loss_box_reg: 0.4289  loss_rpn_cls: 0.02001  loss_rpn_loc: 0.01453    time: 0.4069  last_time: 0.4149  data_time: 0.0273  last_data_time: 0.0066   lr: 0.010789  max_mem: 11505M
[32m[11/10 16:01:07 d2.utils.events]: [0m eta: 0:53:55  iter: 559  total_loss: 0.5384  loss_cls: 0.154  loss_box_reg: 0.3601  loss_rpn_cls: 0.01314  loss_rpn_loc: 0.01012    time: 0.4073  last_time: 0.4316  data_time: 0.0312  last_data_time: 0.0321   lr: 0.011189  max_mem: 11505M
[32m[11/10 16:01:16 d2.utils.events]: [0m eta: 0:53:55  iter: 579  total_loss: 0.5247  loss_cls: 0.1623  loss_box_reg: 0.3249  loss_rpn_cls: 0.01403  loss_rpn_loc: 0.009364    time: 0.4076  last_time: 0.4650  data_time: 0.0302  last_data_time: 0.0371   lr: 0.011588  max_mem: 11505M
[32m[11/10 16:01:24 d2.utils.events]: [0m eta: 0:53:49  iter: 599  total_loss: 0.5371  loss_cls: 0.1379  loss_box_reg: 0.3683  loss_rpn_cls: 0.01398  loss_rpn_loc: 0.01229    time: 0.4080  last_time: 0.3592  data_time: 0.0262  last_data_time: 0.0102   lr: 0.011988  max_mem: 11505M
[32m[11/10 16:01:32 d2.utils.events]: [0m eta: 0:53:43  iter: 619  total_loss: 0.5891  loss_cls: 0.1755  loss_box_reg: 0.3854  loss_rpn_cls: 0.012  loss_rpn_loc: 0.009237    time: 0.4084  last_time: 0.3980  data_time: 0.0281  last_data_time: 0.0214   lr: 0.012388  max_mem: 11505M
[32m[11/10 16:01:40 d2.utils.events]: [0m eta: 0:53:35  iter: 639  total_loss: 0.5261  loss_cls: 0.1619  loss_box_reg: 0.3316  loss_rpn_cls: 0.01353  loss_rpn_loc: 0.01208    time: 0.4084  last_time: 0.4134  data_time: 0.0280  last_data_time: 0.0442   lr: 0.012787  max_mem: 11505M
[32m[11/10 16:01:49 d2.utils.events]: [0m eta: 0:53:27  iter: 659  total_loss: 0.6067  loss_cls: 0.1956  loss_box_reg: 0.3832  loss_rpn_cls: 0.01426  loss_rpn_loc: 0.01142    time: 0.4086  last_time: 0.4504  data_time: 0.0359  last_data_time: 0.0476   lr: 0.013187  max_mem: 11505M
[32m[11/10 16:01:57 d2.utils.events]: [0m eta: 0:53:18  iter: 679  total_loss: 0.5434  loss_cls: 0.1732  loss_box_reg: 0.3431  loss_rpn_cls: 0.01523  loss_rpn_loc: 0.009903    time: 0.4086  last_time: 0.4281  data_time: 0.0300  last_data_time: 0.0447   lr: 0.013586  max_mem: 11505M
[32m[11/10 16:02:05 d2.utils.events]: [0m eta: 0:53:10  iter: 699  total_loss: 0.5099  loss_cls: 0.1438  loss_box_reg: 0.3356  loss_rpn_cls: 0.01372  loss_rpn_loc: 0.01168    time: 0.4088  last_time: 0.4537  data_time: 0.0322  last_data_time: 0.0498   lr: 0.013986  max_mem: 11505M
[32m[11/10 16:02:14 d2.utils.events]: [0m eta: 0:53:04  iter: 719  total_loss: 0.6191  loss_cls: 0.1816  loss_box_reg: 0.3991  loss_rpn_cls: 0.01427  loss_rpn_loc: 0.0136    time: 0.4089  last_time: 0.4608  data_time: 0.0355  last_data_time: 0.0736   lr: 0.014386  max_mem: 11505M
[32m[11/10 16:02:22 d2.utils.events]: [0m eta: 0:52:57  iter: 739  total_loss: 0.6105  loss_cls: 0.1843  loss_box_reg: 0.3689  loss_rpn_cls: 0.01238  loss_rpn_loc: 0.01129    time: 0.4093  last_time: 0.4090  data_time: 0.0335  last_data_time: 0.0207   lr: 0.014785  max_mem: 11505M
[32m[11/10 16:02:31 d2.utils.events]: [0m eta: 0:52:51  iter: 759  total_loss: 0.5722  loss_cls: 0.1844  loss_box_reg: 0.3498  loss_rpn_cls: 0.01525  loss_rpn_loc: 0.01123    time: 0.4099  last_time: 0.4274  data_time: 0.0346  last_data_time: 0.0446   lr: 0.015185  max_mem: 11505M
[32m[11/10 16:02:39 d2.utils.events]: [0m eta: 0:52:43  iter: 779  total_loss: 0.5481  loss_cls: 0.1579  loss_box_reg: 0.354  loss_rpn_cls: 0.01101  loss_rpn_loc: 0.008863    time: 0.4099  last_time: 0.4370  data_time: 0.0344  last_data_time: 0.0272   lr: 0.015584  max_mem: 11505M
[32m[11/10 16:02:47 d2.utils.events]: [0m eta: 0:52:37  iter: 799  total_loss: 0.6067  loss_cls: 0.1771  loss_box_reg: 0.3879  loss_rpn_cls: 0.01422  loss_rpn_loc: 0.01244    time: 0.4102  last_time: 0.4334  data_time: 0.0298  last_data_time: 0.0123   lr: 0.015984  max_mem: 11505M
[32m[11/10 16:02:55 d2.utils.events]: [0m eta: 0:52:28  iter: 819  total_loss: 0.5846  loss_cls: 0.1819  loss_box_reg: 0.3526  loss_rpn_cls: 0.01306  loss_rpn_loc: 0.01306    time: 0.4101  last_time: 0.4242  data_time: 0.0301  last_data_time: 0.0538   lr: 0.016384  max_mem: 11505M
[32m[11/10 16:03:04 d2.utils.events]: [0m eta: 0:52:21  iter: 839  total_loss: 0.5599  loss_cls: 0.1718  loss_box_reg: 0.3522  loss_rpn_cls: 0.01498  loss_rpn_loc: 0.008975    time: 0.4104  last_time: 0.3434  data_time: 0.0330  last_data_time: 0.0263   lr: 0.016783  max_mem: 11505M
[32m[11/10 16:03:12 d2.utils.events]: [0m eta: 0:52:12  iter: 859  total_loss: 0.6159  loss_cls: 0.2038  loss_box_reg: 0.3908  loss_rpn_cls: 0.0138  loss_rpn_loc: 0.01322    time: 0.4104  last_time: 0.4565  data_time: 0.0355  last_data_time: 0.0470   lr: 0.017183  max_mem: 11505M
[32m[11/10 16:03:21 d2.utils.events]: [0m eta: 0:52:05  iter: 879  total_loss: 0.604  loss_cls: 0.1926  loss_box_reg: 0.3935  loss_rpn_cls: 0.01763  loss_rpn_loc: 0.01015    time: 0.4107  last_time: 0.5108  data_time: 0.0309  last_data_time: 0.0527   lr: 0.017582  max_mem: 11505M
[32m[11/10 16:03:29 d2.utils.events]: [0m eta: 0:51:56  iter: 899  total_loss: 0.6288  loss_cls: 0.1932  loss_box_reg: 0.4063  loss_rpn_cls: 0.01447  loss_rpn_loc: 0.01182    time: 0.4108  last_time: 0.5047  data_time: 0.0325  last_data_time: 0.1004   lr: 0.017982  max_mem: 11505M
[32m[11/10 16:03:37 d2.utils.events]: [0m eta: 0:51:50  iter: 919  total_loss: 0.567  loss_cls: 0.1681  loss_box_reg: 0.3681  loss_rpn_cls: 0.01256  loss_rpn_loc: 0.01482    time: 0.4111  last_time: 0.3742  data_time: 0.0292  last_data_time: 0.0109   lr: 0.018382  max_mem: 11505M
[32m[11/10 16:03:46 d2.utils.events]: [0m eta: 0:51:43  iter: 939  total_loss: 0.6048  loss_cls: 0.2044  loss_box_reg: 0.358  loss_rpn_cls: 0.01214  loss_rpn_loc: 0.009773    time: 0.4114  last_time: 0.4410  data_time: 0.0315  last_data_time: 0.0283   lr: 0.018781  max_mem: 11505M
[32m[11/10 16:03:54 d2.utils.events]: [0m eta: 0:51:36  iter: 959  total_loss: 0.6213  loss_cls: 0.2119  loss_box_reg: 0.3798  loss_rpn_cls: 0.0196  loss_rpn_loc: 0.01017    time: 0.4115  last_time: 0.4336  data_time: 0.0317  last_data_time: 0.0393   lr: 0.019181  max_mem: 11505M
[32m[11/10 16:04:03 d2.utils.events]: [0m eta: 0:51:28  iter: 979  total_loss: 0.5949  loss_cls: 0.1993  loss_box_reg: 0.3578  loss_rpn_cls: 0.01953  loss_rpn_loc: 0.01039    time: 0.4116  last_time: 0.4063  data_time: 0.0307  last_data_time: 0.0293   lr: 0.01958  max_mem: 11505M
[32m[11/10 16:04:11 d2.utils.events]: [0m eta: 0:51:18  iter: 999  total_loss: 0.5366  loss_cls: 0.1787  loss_box_reg: 0.3601  loss_rpn_cls: 0.01552  loss_rpn_loc: 0.01199    time: 0.4114  last_time: 0.3768  data_time: 0.0306  last_data_time: 0.0062   lr: 0.01998  max_mem: 11505M
[32m[11/10 16:04:19 d2.utils.events]: [0m eta: 0:51:18  iter: 1019  total_loss: 0.6226  loss_cls: 0.1982  loss_box_reg: 0.387  loss_rpn_cls: 0.01246  loss_rpn_loc: 0.009437    time: 0.4115  last_time: 0.4268  data_time: 0.0316  last_data_time: 0.0069   lr: 0.02  max_mem: 11505M
[32m[11/10 16:04:27 d2.utils.events]: [0m eta: 0:51:16  iter: 1039  total_loss: 0.5536  loss_cls: 0.1585  loss_box_reg: 0.3544  loss_rpn_cls: 0.01465  loss_rpn_loc: 0.01188    time: 0.4117  last_time: 0.4473  data_time: 0.0348  last_data_time: 0.0385   lr: 0.02  max_mem: 11505M
[32m[11/10 16:04:36 d2.utils.events]: [0m eta: 0:51:15  iter: 1059  total_loss: 0.5566  loss_cls: 0.1765  loss_box_reg: 0.3623  loss_rpn_cls: 0.01394  loss_rpn_loc: 0.01061    time: 0.4119  last_time: 0.3661  data_time: 0.0362  last_data_time: 0.0254   lr: 0.02  max_mem: 11505M
[32m[11/10 16:04:44 d2.utils.events]: [0m eta: 0:51:16  iter: 1079  total_loss: 0.5101  loss_cls: 0.1778  loss_box_reg: 0.3109  loss_rpn_cls: 0.01427  loss_rpn_loc: 0.008797    time: 0.4120  last_time: 0.4224  data_time: 0.0346  last_data_time: 0.0302   lr: 0.02  max_mem: 11505M
[32m[11/10 16:04:52 d2.utils.events]: [0m eta: 0:51:10  iter: 1099  total_loss: 0.5825  loss_cls: 0.1807  loss_box_reg: 0.374  loss_rpn_cls: 0.01376  loss_rpn_loc: 0.009894    time: 0.4120  last_time: 0.4163  data_time: 0.0332  last_data_time: 0.0227   lr: 0.02  max_mem: 11505M
[32m[11/10 16:05:01 d2.utils.events]: [0m eta: 0:51:01  iter: 1119  total_loss: 0.6099  loss_cls: 0.1813  loss_box_reg: 0.4111  loss_rpn_cls: 0.01234  loss_rpn_loc: 0.01106    time: 0.4120  last_time: 0.4571  data_time: 0.0310  last_data_time: 0.0517   lr: 0.02  max_mem: 11505M
[32m[11/10 16:05:09 d2.utils.events]: [0m eta: 0:50:53  iter: 1139  total_loss: 0.5342  loss_cls: 0.1655  loss_box_reg: 0.3252  loss_rpn_cls: 0.01482  loss_rpn_loc: 0.01148    time: 0.4121  last_time: 0.3935  data_time: 0.0335  last_data_time: 0.0100   lr: 0.02  max_mem: 11505M
[32m[11/10 16:05:17 d2.utils.events]: [0m eta: 0:50:44  iter: 1159  total_loss: 0.5474  loss_cls: 0.1617  loss_box_reg: 0.3526  loss_rpn_cls: 0.01652  loss_rpn_loc: 0.01297    time: 0.4120  last_time: 0.4804  data_time: 0.0291  last_data_time: 0.0272   lr: 0.02  max_mem: 11505M
[32m[11/10 16:05:26 d2.utils.events]: [0m eta: 0:50:36  iter: 1179  total_loss: 0.5265  loss_cls: 0.1567  loss_box_reg: 0.3384  loss_rpn_cls: 0.01027  loss_rpn_loc: 0.009344    time: 0.4121  last_time: 0.4183  data_time: 0.0331  last_data_time: 0.0148   lr: 0.02  max_mem: 11505M
[32m[11/10 16:05:34 d2.utils.events]: [0m eta: 0:50:27  iter: 1199  total_loss: 0.5673  loss_cls: 0.1756  loss_box_reg: 0.3638  loss_rpn_cls: 0.01712  loss_rpn_loc: 0.01142    time: 0.4122  last_time: 0.4369  data_time: 0.0322  last_data_time: 0.0268   lr: 0.02  max_mem: 11505M
[32m[11/10 16:05:42 d2.utils.events]: [0m eta: 0:50:18  iter: 1219  total_loss: 0.5891  loss_cls: 0.1668  loss_box_reg: 0.3542  loss_rpn_cls: 0.01333  loss_rpn_loc: 0.0118    time: 0.4124  last_time: 0.4129  data_time: 0.0343  last_data_time: 0.0344   lr: 0.02  max_mem: 11505M
[32m[11/10 16:05:51 d2.utils.events]: [0m eta: 0:50:10  iter: 1239  total_loss: 0.4741  loss_cls: 0.1339  loss_box_reg: 0.3109  loss_rpn_cls: 0.01296  loss_rpn_loc: 0.01043    time: 0.4125  last_time: 0.4594  data_time: 0.0314  last_data_time: 0.0279   lr: 0.02  max_mem: 11505M
[32m[11/10 16:05:59 d2.utils.events]: [0m eta: 0:50:02  iter: 1259  total_loss: 0.4858  loss_cls: 0.135  loss_box_reg: 0.3279  loss_rpn_cls: 0.01104  loss_rpn_loc: 0.01105    time: 0.4125  last_time: 0.4239  data_time: 0.0323  last_data_time: 0.0802   lr: 0.02  max_mem: 11505M
[32m[11/10 16:06:08 d2.utils.events]: [0m eta: 0:49:54  iter: 1279  total_loss: 0.5916  loss_cls: 0.1774  loss_box_reg: 0.3957  loss_rpn_cls: 0.01107  loss_rpn_loc: 0.01176    time: 0.4128  last_time: 0.4100  data_time: 0.0336  last_data_time: 0.0263   lr: 0.02  max_mem: 11505M
[32m[11/10 16:06:16 d2.utils.events]: [0m eta: 0:49:49  iter: 1299  total_loss: 0.4985  loss_cls: 0.1581  loss_box_reg: 0.3292  loss_rpn_cls: 0.01206  loss_rpn_loc: 0.009544    time: 0.4130  last_time: 0.4228  data_time: 0.0290  last_data_time: 0.0110   lr: 0.02  max_mem: 11505M
[32m[11/10 16:06:24 d2.utils.events]: [0m eta: 0:49:41  iter: 1319  total_loss: 0.5485  loss_cls: 0.1717  loss_box_reg: 0.3598  loss_rpn_cls: 0.009373  loss_rpn_loc: 0.009729    time: 0.4129  last_time: 0.3595  data_time: 0.0310  last_data_time: 0.0169   lr: 0.02  max_mem: 11505M
[32m[11/10 16:06:33 d2.utils.events]: [0m eta: 0:49:35  iter: 1339  total_loss: 0.5762  loss_cls: 0.18  loss_box_reg: 0.3653  loss_rpn_cls: 0.01367  loss_rpn_loc: 0.01029    time: 0.4132  last_time: 0.4194  data_time: 0.0312  last_data_time: 0.0337   lr: 0.02  max_mem: 11505M
[32m[11/10 16:06:41 d2.utils.events]: [0m eta: 0:49:29  iter: 1359  total_loss: 0.6009  loss_cls: 0.1841  loss_box_reg: 0.3665  loss_rpn_cls: 0.01485  loss_rpn_loc: 0.01087    time: 0.4133  last_time: 0.4273  data_time: 0.0326  last_data_time: 0.0623   lr: 0.02  max_mem: 11505M
[32m[11/10 16:06:50 d2.utils.events]: [0m eta: 0:49:23  iter: 1379  total_loss: 0.5005  loss_cls: 0.1543  loss_box_reg: 0.326  loss_rpn_cls: 0.0115  loss_rpn_loc: 0.007995    time: 0.4134  last_time: 0.4120  data_time: 0.0301  last_data_time: 0.0293   lr: 0.02  max_mem: 11505M
[32m[11/10 16:06:58 d2.utils.events]: [0m eta: 0:49:16  iter: 1399  total_loss: 0.4617  loss_cls: 0.1302  loss_box_reg: 0.2919  loss_rpn_cls: 0.01267  loss_rpn_loc: 0.01073    time: 0.4134  last_time: 0.4262  data_time: 0.0334  last_data_time: 0.0402   lr: 0.02  max_mem: 11505M
[32m[11/10 16:07:06 d2.utils.events]: [0m eta: 0:49:07  iter: 1419  total_loss: 0.4918  loss_cls: 0.1741  loss_box_reg: 0.3008  loss_rpn_cls: 0.009837  loss_rpn_loc: 0.008601    time: 0.4133  last_time: 0.3977  data_time: 0.0318  last_data_time: 0.0543   lr: 0.02  max_mem: 11505M
[32m[11/10 16:07:15 d2.utils.events]: [0m eta: 0:48:58  iter: 1439  total_loss: 0.5719  loss_cls: 0.1771  loss_box_reg: 0.3673  loss_rpn_cls: 0.01158  loss_rpn_loc: 0.009167    time: 0.4134  last_time: 0.4514  data_time: 0.0318  last_data_time: 0.0407   lr: 0.02  max_mem: 11505M
[32m[11/10 16:07:23 d2.utils.events]: [0m eta: 0:48:53  iter: 1459  total_loss: 0.5608  loss_cls: 0.1747  loss_box_reg: 0.3825  loss_rpn_cls: 0.009264  loss_rpn_loc: 0.009575    time: 0.4135  last_time: 0.4313  data_time: 0.0364  last_data_time: 0.0086   lr: 0.02  max_mem: 11505M
[32m[11/10 16:07:31 d2.utils.events]: [0m eta: 0:48:46  iter: 1479  total_loss: 0.5056  loss_cls: 0.1584  loss_box_reg: 0.329  loss_rpn_cls: 0.01189  loss_rpn_loc: 0.009462    time: 0.4136  last_time: 0.3639  data_time: 0.0293  last_data_time: 0.0069   lr: 0.02  max_mem: 11505M
[32m[11/10 16:07:40 d2.utils.events]: [0m eta: 0:48:42  iter: 1499  total_loss: 0.5335  loss_cls: 0.1709  loss_box_reg: 0.3566  loss_rpn_cls: 0.0109  loss_rpn_loc: 0.007819    time: 0.4136  last_time: 0.4386  data_time: 0.0327  last_data_time: 0.0300   lr: 0.02  max_mem: 11505M
[32m[11/10 16:07:48 d2.utils.events]: [0m eta: 0:48:38  iter: 1519  total_loss: 0.5304  loss_cls: 0.1619  loss_box_reg: 0.3333  loss_rpn_cls: 0.00826  loss_rpn_loc: 0.01047    time: 0.4138  last_time: 0.4434  data_time: 0.0310  last_data_time: 0.0075   lr: 0.02  max_mem: 11505M
[32m[11/10 16:07:57 d2.utils.events]: [0m eta: 0:48:30  iter: 1539  total_loss: 0.4853  loss_cls: 0.1481  loss_box_reg: 0.3395  loss_rpn_cls: 0.008191  loss_rpn_loc: 0.009464    time: 0.4140  last_time: 0.5169  data_time: 0.0364  last_data_time: 0.1000   lr: 0.02  max_mem: 11505M
[32m[11/10 16:08:05 d2.utils.events]: [0m eta: 0:48:22  iter: 1559  total_loss: 0.5637  loss_cls: 0.1903  loss_box_reg: 0.3502  loss_rpn_cls: 0.01464  loss_rpn_loc: 0.01339    time: 0.4141  last_time: 0.4039  data_time: 0.0337  last_data_time: 0.0362   lr: 0.02  max_mem: 11505M
[32m[11/10 16:08:14 d2.utils.events]: [0m eta: 0:48:18  iter: 1579  total_loss: 0.5802  loss_cls: 0.1691  loss_box_reg: 0.3698  loss_rpn_cls: 0.01363  loss_rpn_loc: 0.009286    time: 0.4143  last_time: 0.4903  data_time: 0.0386  last_data_time: 0.0452   lr: 0.02  max_mem: 11505M
[32m[11/10 16:08:22 d2.utils.events]: [0m eta: 0:48:09  iter: 1599  total_loss: 0.5519  loss_cls: 0.1692  loss_box_reg: 0.382  loss_rpn_cls: 0.01049  loss_rpn_loc: 0.01132    time: 0.4143  last_time: 0.3570  data_time: 0.0310  last_data_time: 0.0240   lr: 0.02  max_mem: 11505M
[32m[11/10 16:08:31 d2.utils.events]: [0m eta: 0:48:01  iter: 1619  total_loss: 0.5292  loss_cls: 0.166  loss_box_reg: 0.3383  loss_rpn_cls: 0.01168  loss_rpn_loc: 0.008162    time: 0.4144  last_time: 0.4477  data_time: 0.0318  last_data_time: 0.0588   lr: 0.02  max_mem: 11505M
[32m[11/10 16:08:39 d2.utils.events]: [0m eta: 0:47:56  iter: 1639  total_loss: 0.5332  loss_cls: 0.1599  loss_box_reg: 0.3385  loss_rpn_cls: 0.01301  loss_rpn_loc: 0.01062    time: 0.4146  last_time: 0.4569  data_time: 0.0379  last_data_time: 0.0320   lr: 0.02  max_mem: 11505M
[32m[11/10 16:08:48 d2.utils.events]: [0m eta: 0:47:49  iter: 1659  total_loss: 0.5555  loss_cls: 0.1678  loss_box_reg: 0.3582  loss_rpn_cls: 0.01039  loss_rpn_loc: 0.007849    time: 0.4148  last_time: 0.4662  data_time: 0.0338  last_data_time: 0.0294   lr: 0.02  max_mem: 11505M
[32m[11/10 16:08:56 d2.utils.events]: [0m eta: 0:47:41  iter: 1679  total_loss: 0.514  loss_cls: 0.1439  loss_box_reg: 0.3496  loss_rpn_cls: 0.00637  loss_rpn_loc: 0.008504    time: 0.4148  last_time: 0.3964  data_time: 0.0336  last_data_time: 0.0041   lr: 0.02  max_mem: 11505M
[32m[11/10 16:09:05 d2.utils.events]: [0m eta: 0:47:33  iter: 1699  total_loss: 0.5376  loss_cls: 0.1577  loss_box_reg: 0.3428  loss_rpn_cls: 0.01176  loss_rpn_loc: 0.01115    time: 0.4148  last_time: 0.4160  data_time: 0.0307  last_data_time: 0.0227   lr: 0.02  max_mem: 11505M
[32m[11/10 16:09:13 d2.utils.events]: [0m eta: 0:47:24  iter: 1719  total_loss: 0.518  loss_cls: 0.148  loss_box_reg: 0.3254  loss_rpn_cls: 0.01195  loss_rpn_loc: 0.008971    time: 0.4150  last_time: 0.5205  data_time: 0.0370  last_data_time: 0.0892   lr: 0.02  max_mem: 11505M
[32m[11/10 16:09:22 d2.utils.events]: [0m eta: 0:47:16  iter: 1739  total_loss: 0.4826  loss_cls: 0.1433  loss_box_reg: 0.3208  loss_rpn_cls: 0.0112  loss_rpn_loc: 0.009813    time: 0.4151  last_time: 0.4191  data_time: 0.0303  last_data_time: 0.0187   lr: 0.02  max_mem: 11505M
[32m[11/10 16:09:30 d2.utils.events]: [0m eta: 0:47:07  iter: 1759  total_loss: 0.5028  loss_cls: 0.1429  loss_box_reg: 0.3437  loss_rpn_cls: 0.009089  loss_rpn_loc: 0.009515    time: 0.4152  last_time: 0.4587  data_time: 0.0320  last_data_time: 0.0314   lr: 0.02  max_mem: 11505M
[32m[11/10 16:09:39 d2.utils.events]: [0m eta: 0:47:00  iter: 1779  total_loss: 0.5106  loss_cls: 0.1592  loss_box_reg: 0.3153  loss_rpn_cls: 0.01041  loss_rpn_loc: 0.008789    time: 0.4154  last_time: 0.4506  data_time: 0.0317  last_data_time: 0.0269   lr: 0.02  max_mem: 11505M
[32m[11/10 16:09:47 d2.utils.events]: [0m eta: 0:46:55  iter: 1799  total_loss: 0.4984  loss_cls: 0.1349  loss_box_reg: 0.3117  loss_rpn_cls: 0.01036  loss_rpn_loc: 0.01066    time: 0.4155  last_time: 0.4627  data_time: 0.0355  last_data_time: 0.0470   lr: 0.02  max_mem: 11505M
[32m[11/10 16:09:56 d2.utils.events]: [0m eta: 0:46:46  iter: 1819  total_loss: 0.5149  loss_cls: 0.1556  loss_box_reg: 0.348  loss_rpn_cls: 0.01144  loss_rpn_loc: 0.009287    time: 0.4156  last_time: 0.3987  data_time: 0.0362  last_data_time: 0.0197   lr: 0.02  max_mem: 11505M
[32m[11/10 16:10:04 d2.utils.events]: [0m eta: 0:46:35  iter: 1839  total_loss: 0.5003  loss_cls: 0.1409  loss_box_reg: 0.3321  loss_rpn_cls: 0.01007  loss_rpn_loc: 0.008785    time: 0.4156  last_time: 0.3514  data_time: 0.0298  last_data_time: 0.0377   lr: 0.02  max_mem: 11505M
[32m[11/10 16:10:12 d2.utils.events]: [0m eta: 0:46:27  iter: 1859  total_loss: 0.4704  loss_cls: 0.1321  loss_box_reg: 0.3164  loss_rpn_cls: 0.009753  loss_rpn_loc: 0.008645    time: 0.4156  last_time: 0.4401  data_time: 0.0312  last_data_time: 0.0078   lr: 0.02  max_mem: 11505M
[32m[11/10 16:10:21 d2.utils.events]: [0m eta: 0:46:20  iter: 1879  total_loss: 0.4621  loss_cls: 0.1375  loss_box_reg: 0.3115  loss_rpn_cls: 0.01025  loss_rpn_loc: 0.008692    time: 0.4157  last_time: 0.4024  data_time: 0.0304  last_data_time: 0.0289   lr: 0.02  max_mem: 11505M
[32m[11/10 16:10:29 d2.utils.events]: [0m eta: 0:46:12  iter: 1899  total_loss: 0.4942  loss_cls: 0.1579  loss_box_reg: 0.3383  loss_rpn_cls: 0.008936  loss_rpn_loc: 0.008363    time: 0.4157  last_time: 0.4146  data_time: 0.0327  last_data_time: 0.0469   lr: 0.02  max_mem: 11505M
[32m[11/10 16:10:38 d2.utils.events]: [0m eta: 0:46:04  iter: 1919  total_loss: 0.492  loss_cls: 0.1428  loss_box_reg: 0.321  loss_rpn_cls: 0.009057  loss_rpn_loc: 0.01054    time: 0.4157  last_time: 0.3985  data_time: 0.0335  last_data_time: 0.0058   lr: 0.02  max_mem: 11505M
[32m[11/10 16:10:46 d2.utils.events]: [0m eta: 0:45:56  iter: 1939  total_loss: 0.4876  loss_cls: 0.1353  loss_box_reg: 0.3189  loss_rpn_cls: 0.008642  loss_rpn_loc: 0.01029    time: 0.4158  last_time: 0.4036  data_time: 0.0309  last_data_time: 0.0173   lr: 0.02  max_mem: 11505M
[32m[11/10 16:10:55 d2.utils.events]: [0m eta: 0:45:45  iter: 1959  total_loss: 0.4834  loss_cls: 0.1491  loss_box_reg: 0.3413  loss_rpn_cls: 0.007928  loss_rpn_loc: 0.007777    time: 0.4158  last_time: 0.4421  data_time: 0.0357  last_data_time: 0.0663   lr: 0.02  max_mem: 11505M
[32m[11/10 16:11:03 d2.utils.events]: [0m eta: 0:45:40  iter: 1979  total_loss: 0.4763  loss_cls: 0.1274  loss_box_reg: 0.3251  loss_rpn_cls: 0.009488  loss_rpn_loc: 0.009827    time: 0.4159  last_time: 0.4773  data_time: 0.0324  last_data_time: 0.0382   lr: 0.02  max_mem: 11505M
[32m[11/10 16:11:12 d2.utils.events]: [0m eta: 0:45:35  iter: 1999  total_loss: 0.506  loss_cls: 0.1541  loss_box_reg: 0.3167  loss_rpn_cls: 0.009888  loss_rpn_loc: 0.00957    time: 0.4160  last_time: 0.4588  data_time: 0.0339  last_data_time: 0.0439   lr: 0.02  max_mem: 11505M
[32m[11/10 16:11:20 d2.utils.events]: [0m eta: 0:45:27  iter: 2019  total_loss: 0.508  loss_cls: 0.1614  loss_box_reg: 0.3365  loss_rpn_cls: 0.01102  loss_rpn_loc: 0.008972    time: 0.4160  last_time: 0.4442  data_time: 0.0309  last_data_time: 0.0354   lr: 0.02  max_mem: 11505M
[32m[11/10 16:11:28 d2.utils.events]: [0m eta: 0:45:18  iter: 2039  total_loss: 0.5484  loss_cls: 0.1726  loss_box_reg: 0.3309  loss_rpn_cls: 0.01012  loss_rpn_loc: 0.008267    time: 0.4160  last_time: 0.3908  data_time: 0.0351  last_data_time: 0.0199   lr: 0.02  max_mem: 11505M
[32m[11/10 16:11:37 d2.utils.events]: [0m eta: 0:45:10  iter: 2059  total_loss: 0.5191  loss_cls: 0.1525  loss_box_reg: 0.3543  loss_rpn_cls: 0.01402  loss_rpn_loc: 0.01087    time: 0.4161  last_time: 0.4610  data_time: 0.0379  last_data_time: 0.0639   lr: 0.02  max_mem: 11505M
[32m[11/10 16:11:45 d2.utils.events]: [0m eta: 0:44:59  iter: 2079  total_loss: 0.5705  loss_cls: 0.1829  loss_box_reg: 0.3571  loss_rpn_cls: 0.01104  loss_rpn_loc: 0.009623    time: 0.4160  last_time: 0.3955  data_time: 0.0324  last_data_time: 0.0278   lr: 0.02  max_mem: 11505M
[32m[11/10 16:11:53 d2.utils.events]: [0m eta: 0:44:52  iter: 2099  total_loss: 0.4956  loss_cls: 0.1434  loss_box_reg: 0.3123  loss_rpn_cls: 0.01378  loss_rpn_loc: 0.01234    time: 0.4161  last_time: 0.4016  data_time: 0.0311  last_data_time: 0.0193   lr: 0.02  max_mem: 11505M
[32m[11/10 16:12:02 d2.utils.events]: [0m eta: 0:44:44  iter: 2119  total_loss: 0.5312  loss_cls: 0.1661  loss_box_reg: 0.3429  loss_rpn_cls: 0.01127  loss_rpn_loc: 0.009781    time: 0.4161  last_time: 0.4092  data_time: 0.0380  last_data_time: 0.0607   lr: 0.02  max_mem: 11505M
[32m[11/10 16:12:10 d2.utils.events]: [0m eta: 0:44:36  iter: 2139  total_loss: 0.4829  loss_cls: 0.1432  loss_box_reg: 0.3129  loss_rpn_cls: 0.009029  loss_rpn_loc: 0.008933    time: 0.4162  last_time: 0.4393  data_time: 0.0335  last_data_time: 0.0409   lr: 0.02  max_mem: 11505M
[32m[11/10 16:12:19 d2.utils.events]: [0m eta: 0:44:30  iter: 2159  total_loss: 0.5115  loss_cls: 0.152  loss_box_reg: 0.3153  loss_rpn_cls: 0.01522  loss_rpn_loc: 0.01068    time: 0.4162  last_time: 0.3699  data_time: 0.0318  last_data_time: 0.0116   lr: 0.02  max_mem: 11505M
[32m[11/10 16:12:27 d2.utils.events]: [0m eta: 0:44:24  iter: 2179  total_loss: 0.5116  loss_cls: 0.1531  loss_box_reg: 0.3429  loss_rpn_cls: 0.009136  loss_rpn_loc: 0.008633    time: 0.4162  last_time: 0.3882  data_time: 0.0310  last_data_time: 0.0431   lr: 0.02  max_mem: 11505M
[32m[11/10 16:12:35 d2.utils.events]: [0m eta: 0:44:18  iter: 2199  total_loss: 0.5287  loss_cls: 0.1487  loss_box_reg: 0.3373  loss_rpn_cls: 0.01121  loss_rpn_loc: 0.01107    time: 0.4163  last_time: 0.4865  data_time: 0.0347  last_data_time: 0.0608   lr: 0.02  max_mem: 11505M
[32m[11/10 16:12:44 d2.utils.events]: [0m eta: 0:44:07  iter: 2219  total_loss: 0.4802  loss_cls: 0.1293  loss_box_reg: 0.32  loss_rpn_cls: 0.007281  loss_rpn_loc: 0.009897    time: 0.4162  last_time: 0.3576  data_time: 0.0355  last_data_time: 0.0077   lr: 0.02  max_mem: 11505M
[32m[11/10 16:12:52 d2.utils.events]: [0m eta: 0:43:59  iter: 2239  total_loss: 0.4997  loss_cls: 0.1422  loss_box_reg: 0.3436  loss_rpn_cls: 0.01033  loss_rpn_loc: 0.01007    time: 0.4163  last_time: 0.4628  data_time: 0.0398  last_data_time: 0.0260   lr: 0.02  max_mem: 11505M
[32m[11/10 16:13:00 d2.utils.events]: [0m eta: 0:43:50  iter: 2259  total_loss: 0.4723  loss_cls: 0.14  loss_box_reg: 0.3119  loss_rpn_cls: 0.008558  loss_rpn_loc: 0.00777    time: 0.4163  last_time: 0.4328  data_time: 0.0336  last_data_time: 0.0437   lr: 0.02  max_mem: 11505M
[32m[11/10 16:13:09 d2.utils.events]: [0m eta: 0:43:42  iter: 2279  total_loss: 0.4606  loss_cls: 0.1333  loss_box_reg: 0.2974  loss_rpn_cls: 0.01006  loss_rpn_loc: 0.01075    time: 0.4163  last_time: 0.3523  data_time: 0.0337  last_data_time: 0.0270   lr: 0.02  max_mem: 11505M
[32m[11/10 16:13:17 d2.utils.events]: [0m eta: 0:43:33  iter: 2299  total_loss: 0.4675  loss_cls: 0.122  loss_box_reg: 0.3289  loss_rpn_cls: 0.01013  loss_rpn_loc: 0.01007    time: 0.4164  last_time: 0.3903  data_time: 0.0408  last_data_time: 0.0376   lr: 0.02  max_mem: 11505M
[32m[11/10 16:13:26 d2.utils.events]: [0m eta: 0:43:24  iter: 2319  total_loss: 0.5458  loss_cls: 0.1762  loss_box_reg: 0.3356  loss_rpn_cls: 0.009262  loss_rpn_loc: 0.008884    time: 0.4164  last_time: 0.3913  data_time: 0.0312  last_data_time: 0.0064   lr: 0.02  max_mem: 11505M
[32m[11/10 16:13:34 d2.utils.events]: [0m eta: 0:43:15  iter: 2339  total_loss: 0.4616  loss_cls: 0.1405  loss_box_reg: 0.2965  loss_rpn_cls: 0.008472  loss_rpn_loc: 0.009037    time: 0.4165  last_time: 0.3672  data_time: 0.0329  last_data_time: 0.0087   lr: 0.02  max_mem: 11505M
[32m[11/10 16:13:43 d2.utils.events]: [0m eta: 0:43:03  iter: 2359  total_loss: 0.5072  loss_cls: 0.1335  loss_box_reg: 0.3247  loss_rpn_cls: 0.007738  loss_rpn_loc: 0.0106    time: 0.4166  last_time: 0.4000  data_time: 0.0365  last_data_time: 0.0581   lr: 0.02  max_mem: 11505M
[32m[11/10 16:13:51 d2.utils.events]: [0m eta: 0:42:58  iter: 2379  total_loss: 0.4359  loss_cls: 0.1151  loss_box_reg: 0.3039  loss_rpn_cls: 0.009431  loss_rpn_loc: 0.008814    time: 0.4165  last_time: 0.4365  data_time: 0.0324  last_data_time: 0.0328   lr: 0.02  max_mem: 11505M
[32m[11/10 16:14:00 d2.utils.events]: [0m eta: 0:42:52  iter: 2399  total_loss: 0.47  loss_cls: 0.1325  loss_box_reg: 0.3053  loss_rpn_cls: 0.008081  loss_rpn_loc: 0.009551    time: 0.4167  last_time: 0.4195  data_time: 0.0292  last_data_time: 0.0457   lr: 0.02  max_mem: 11505M
[32m[11/10 16:14:08 d2.utils.events]: [0m eta: 0:42:46  iter: 2419  total_loss: 0.5246  loss_cls: 0.1434  loss_box_reg: 0.3327  loss_rpn_cls: 0.007595  loss_rpn_loc: 0.008038    time: 0.4167  last_time: 0.4260  data_time: 0.0372  last_data_time: 0.0668   lr: 0.02  max_mem: 11505M
[32m[11/10 16:14:16 d2.utils.events]: [0m eta: 0:42:37  iter: 2439  total_loss: 0.4273  loss_cls: 0.1229  loss_box_reg: 0.2808  loss_rpn_cls: 0.008103  loss_rpn_loc: 0.007168    time: 0.4166  last_time: 0.4629  data_time: 0.0333  last_data_time: 0.0390   lr: 0.02  max_mem: 11505M
[32m[11/10 16:14:25 d2.utils.events]: [0m eta: 0:42:29  iter: 2459  total_loss: 0.5024  loss_cls: 0.1542  loss_box_reg: 0.3115  loss_rpn_cls: 0.01057  loss_rpn_loc: 0.01126    time: 0.4168  last_time: 0.4458  data_time: 0.0343  last_data_time: 0.0326   lr: 0.02  max_mem: 11505M
[32m[11/10 16:14:33 d2.utils.events]: [0m eta: 0:42:20  iter: 2479  total_loss: 0.4845  loss_cls: 0.1469  loss_box_reg: 0.3176  loss_rpn_cls: 0.008818  loss_rpn_loc: 0.008308    time: 0.4167  last_time: 0.4504  data_time: 0.0320  last_data_time: 0.0379   lr: 0.02  max_mem: 11505M
[32m[11/10 16:14:42 d2.utils.events]: [0m eta: 0:42:11  iter: 2499  total_loss: 0.4941  loss_cls: 0.1385  loss_box_reg: 0.3222  loss_rpn_cls: 0.009483  loss_rpn_loc: 0.00951    time: 0.4168  last_time: 0.4502  data_time: 0.0368  last_data_time: 0.0603   lr: 0.02  max_mem: 11505M
[32m[11/10 16:14:50 d2.utils.events]: [0m eta: 0:41:59  iter: 2519  total_loss: 0.4925  loss_cls: 0.163  loss_box_reg: 0.3186  loss_rpn_cls: 0.00661  loss_rpn_loc: 0.007354    time: 0.4168  last_time: 0.4143  data_time: 0.0326  last_data_time: 0.0238   lr: 0.02  max_mem: 11505M
[32m[11/10 16:14:58 d2.utils.events]: [0m eta: 0:41:46  iter: 2539  total_loss: 0.4795  loss_cls: 0.1318  loss_box_reg: 0.3189  loss_rpn_cls: 0.009918  loss_rpn_loc: 0.01079    time: 0.4168  last_time: 0.4039  data_time: 0.0347  last_data_time: 0.0418   lr: 0.02  max_mem: 11505M
[32m[11/10 16:15:07 d2.utils.events]: [0m eta: 0:41:35  iter: 2559  total_loss: 0.5134  loss_cls: 0.1559  loss_box_reg: 0.3375  loss_rpn_cls: 0.009353  loss_rpn_loc: 0.009129    time: 0.4168  last_time: 0.4328  data_time: 0.0362  last_data_time: 0.0305   lr: 0.02  max_mem: 11505M
[32m[11/10 16:15:15 d2.utils.events]: [0m eta: 0:41:26  iter: 2579  total_loss: 0.5041  loss_cls: 0.1447  loss_box_reg: 0.3402  loss_rpn_cls: 0.009091  loss_rpn_loc: 0.01024    time: 0.4168  last_time: 0.4198  data_time: 0.0357  last_data_time: 0.0389   lr: 0.02  max_mem: 11505M
[32m[11/10 16:15:23 d2.utils.events]: [0m eta: 0:41:18  iter: 2599  total_loss: 0.4858  loss_cls: 0.1537  loss_box_reg: 0.3148  loss_rpn_cls: 0.008981  loss_rpn_loc: 0.008319    time: 0.4168  last_time: 0.3855  data_time: 0.0376  last_data_time: 0.0099   lr: 0.02  max_mem: 11505M
[32m[11/10 16:15:32 d2.utils.events]: [0m eta: 0:41:09  iter: 2619  total_loss: 0.5068  loss_cls: 0.1349  loss_box_reg: 0.3397  loss_rpn_cls: 0.009285  loss_rpn_loc: 0.00718    time: 0.4168  last_time: 0.4267  data_time: 0.0377  last_data_time: 0.0405   lr: 0.02  max_mem: 11505M
[32m[11/10 16:15:40 d2.utils.events]: [0m eta: 0:41:00  iter: 2639  total_loss: 0.4948  loss_cls: 0.1533  loss_box_reg: 0.3392  loss_rpn_cls: 0.007996  loss_rpn_loc: 0.008957    time: 0.4168  last_time: 0.4541  data_time: 0.0352  last_data_time: 0.0326   lr: 0.02  max_mem: 11505M
[32m[11/10 16:15:49 d2.utils.events]: [0m eta: 0:40:52  iter: 2659  total_loss: 0.4814  loss_cls: 0.1468  loss_box_reg: 0.3232  loss_rpn_cls: 0.01086  loss_rpn_loc: 0.009448    time: 0.4169  last_time: 0.4528  data_time: 0.0316  last_data_time: 0.0350   lr: 0.02  max_mem: 11505M
[32m[11/10 16:15:57 d2.utils.events]: [0m eta: 0:40:45  iter: 2679  total_loss: 0.4684  loss_cls: 0.1404  loss_box_reg: 0.3078  loss_rpn_cls: 0.0114  loss_rpn_loc: 0.007361    time: 0.4169  last_time: 0.3559  data_time: 0.0347  last_data_time: 0.0111   lr: 0.02  max_mem: 11505M
[32m[11/10 16:16:05 d2.utils.events]: [0m eta: 0:40:36  iter: 2699  total_loss: 0.453  loss_cls: 0.127  loss_box_reg: 0.3104  loss_rpn_cls: 0.006991  loss_rpn_loc: 0.008561    time: 0.4169  last_time: 0.4111  data_time: 0.0341  last_data_time: 0.0029   lr: 0.02  max_mem: 11505M
[32m[11/10 16:16:14 d2.utils.events]: [0m eta: 0:40:27  iter: 2719  total_loss: 0.4857  loss_cls: 0.145  loss_box_reg: 0.3347  loss_rpn_cls: 0.007685  loss_rpn_loc: 0.007953    time: 0.4169  last_time: 0.4112  data_time: 0.0326  last_data_time: 0.0343   lr: 0.02  max_mem: 11505M
[32m[11/10 16:16:22 d2.utils.events]: [0m eta: 0:40:19  iter: 2739  total_loss: 0.4893  loss_cls: 0.1443  loss_box_reg: 0.3253  loss_rpn_cls: 0.007452  loss_rpn_loc: 0.01002    time: 0.4170  last_time: 0.4527  data_time: 0.0297  last_data_time: 0.0353   lr: 0.02  max_mem: 11505M
[32m[11/10 16:16:31 d2.utils.events]: [0m eta: 0:40:11  iter: 2759  total_loss: 0.5094  loss_cls: 0.1558  loss_box_reg: 0.3431  loss_rpn_cls: 0.00871  loss_rpn_loc: 0.00789    time: 0.4170  last_time: 0.4285  data_time: 0.0358  last_data_time: 0.0380   lr: 0.02  max_mem: 11505M
[32m[11/10 16:16:39 d2.utils.events]: [0m eta: 0:40:01  iter: 2779  total_loss: 0.4803  loss_cls: 0.1264  loss_box_reg: 0.3243  loss_rpn_cls: 0.008631  loss_rpn_loc: 0.008361    time: 0.4170  last_time: 0.4196  data_time: 0.0319  last_data_time: 0.0092   lr: 0.02  max_mem: 11505M
[32m[11/10 16:16:47 d2.utils.events]: [0m eta: 0:39:51  iter: 2799  total_loss: 0.4497  loss_cls: 0.1234  loss_box_reg: 0.3121  loss_rpn_cls: 0.006529  loss_rpn_loc: 0.008088    time: 0.4170  last_time: 0.3776  data_time: 0.0345  last_data_time: 0.0256   lr: 0.02  max_mem: 11505M
[32m[11/10 16:16:56 d2.utils.events]: [0m eta: 0:39:43  iter: 2819  total_loss: 0.4679  loss_cls: 0.1319  loss_box_reg: 0.3017  loss_rpn_cls: 0.008393  loss_rpn_loc: 0.008351    time: 0.4170  last_time: 0.3956  data_time: 0.0320  last_data_time: 0.0082   lr: 0.02  max_mem: 11505M
[32m[11/10 16:17:04 d2.utils.events]: [0m eta: 0:39:35  iter: 2839  total_loss: 0.4869  loss_cls: 0.1379  loss_box_reg: 0.311  loss_rpn_cls: 0.01153  loss_rpn_loc: 0.009814    time: 0.4170  last_time: 0.3976  data_time: 0.0364  last_data_time: 0.0150   lr: 0.02  max_mem: 11505M
[32m[11/10 16:17:13 d2.utils.events]: [0m eta: 0:39:27  iter: 2859  total_loss: 0.4484  loss_cls: 0.1352  loss_box_reg: 0.2965  loss_rpn_cls: 0.008396  loss_rpn_loc: 0.008344    time: 0.4170  last_time: 0.3883  data_time: 0.0354  last_data_time: 0.0098   lr: 0.02  max_mem: 11505M
[32m[11/10 16:17:21 d2.utils.events]: [0m eta: 0:39:19  iter: 2879  total_loss: 0.4414  loss_cls: 0.1101  loss_box_reg: 0.295  loss_rpn_cls: 0.007322  loss_rpn_loc: 0.00837    time: 0.4171  last_time: 0.4805  data_time: 0.0396  last_data_time: 0.0782   lr: 0.02  max_mem: 11505M
[32m[11/10 16:17:30 d2.utils.events]: [0m eta: 0:39:11  iter: 2899  total_loss: 0.4684  loss_cls: 0.1303  loss_box_reg: 0.3137  loss_rpn_cls: 0.006282  loss_rpn_loc: 0.007766    time: 0.4172  last_time: 0.4983  data_time: 0.0392  last_data_time: 0.1009   lr: 0.02  max_mem: 11505M
[32m[11/10 16:17:38 d2.utils.events]: [0m eta: 0:39:04  iter: 2919  total_loss: 0.5014  loss_cls: 0.1466  loss_box_reg: 0.3201  loss_rpn_cls: 0.00702  loss_rpn_loc: 0.009211    time: 0.4172  last_time: 0.3780  data_time: 0.0320  last_data_time: 0.0651   lr: 0.02  max_mem: 11505M
[32m[11/10 16:17:47 d2.utils.events]: [0m eta: 0:38:56  iter: 2939  total_loss: 0.5095  loss_cls: 0.1171  loss_box_reg: 0.329  loss_rpn_cls: 0.008553  loss_rpn_loc: 0.009444    time: 0.4173  last_time: 0.3900  data_time: 0.0353  last_data_time: 0.0118   lr: 0.02  max_mem: 11505M
[32m[11/10 16:17:55 d2.utils.events]: [0m eta: 0:38:47  iter: 2959  total_loss: 0.4617  loss_cls: 0.126  loss_box_reg: 0.3257  loss_rpn_cls: 0.007853  loss_rpn_loc: 0.008974    time: 0.4173  last_time: 0.4012  data_time: 0.0325  last_data_time: 0.0090   lr: 0.02  max_mem: 11505M
[32m[11/10 16:18:04 d2.utils.events]: [0m eta: 0:38:38  iter: 2979  total_loss: 0.4496  loss_cls: 0.1174  loss_box_reg: 0.3148  loss_rpn_cls: 0.006687  loss_rpn_loc: 0.007534    time: 0.4174  last_time: 0.4148  data_time: 0.0364  last_data_time: 0.0222   lr: 0.02  max_mem: 11505M
[32m[11/10 16:18:12 d2.utils.events]: [0m eta: 0:38:28  iter: 2999  total_loss: 0.495  loss_cls: 0.1219  loss_box_reg: 0.3362  loss_rpn_cls: 0.00751  loss_rpn_loc: 0.007479    time: 0.4173  last_time: 0.4382  data_time: 0.0323  last_data_time: 0.0698   lr: 0.02  max_mem: 11505M
[32m[11/10 16:18:20 d2.utils.events]: [0m eta: 0:38:20  iter: 3019  total_loss: 0.5175  loss_cls: 0.1314  loss_box_reg: 0.3505  loss_rpn_cls: 0.008201  loss_rpn_loc: 0.006336    time: 0.4174  last_time: 0.4343  data_time: 0.0356  last_data_time: 0.0126   lr: 0.02  max_mem: 11505M
[32m[11/10 16:18:29 d2.utils.events]: [0m eta: 0:38:11  iter: 3039  total_loss: 0.4961  loss_cls: 0.1425  loss_box_reg: 0.3364  loss_rpn_cls: 0.01188  loss_rpn_loc: 0.008856    time: 0.4174  last_time: 0.3910  data_time: 0.0382  last_data_time: 0.0525   lr: 0.02  max_mem: 11505M
[32m[11/10 16:18:37 d2.utils.events]: [0m eta: 0:38:03  iter: 3059  total_loss: 0.4033  loss_cls: 0.1011  loss_box_reg: 0.2843  loss_rpn_cls: 0.009723  loss_rpn_loc: 0.008211    time: 0.4174  last_time: 0.3467  data_time: 0.0326  last_data_time: 0.0076   lr: 0.02  max_mem: 11505M
[32m[11/10 16:18:46 d2.utils.events]: [0m eta: 0:37:55  iter: 3079  total_loss: 0.4792  loss_cls: 0.1308  loss_box_reg: 0.3109  loss_rpn_cls: 0.01083  loss_rpn_loc: 0.007685    time: 0.4175  last_time: 0.3856  data_time: 0.0350  last_data_time: 0.0439   lr: 0.02  max_mem: 11505M
[32m[11/10 16:18:54 d2.utils.events]: [0m eta: 0:37:46  iter: 3099  total_loss: 0.4983  loss_cls: 0.1547  loss_box_reg: 0.3279  loss_rpn_cls: 0.008783  loss_rpn_loc: 0.006242    time: 0.4174  last_time: 0.3806  data_time: 0.0318  last_data_time: 0.0095   lr: 0.02  max_mem: 11505M
[32m[11/10 16:19:02 d2.utils.events]: [0m eta: 0:37:38  iter: 3119  total_loss: 0.4526  loss_cls: 0.1278  loss_box_reg: 0.311  loss_rpn_cls: 0.008855  loss_rpn_loc: 0.008891    time: 0.4174  last_time: 0.4934  data_time: 0.0360  last_data_time: 0.0675   lr: 0.02  max_mem: 11505M
[32m[11/10 16:19:11 d2.utils.events]: [0m eta: 0:37:29  iter: 3139  total_loss: 0.4534  loss_cls: 0.1271  loss_box_reg: 0.306  loss_rpn_cls: 0.007931  loss_rpn_loc: 0.009099    time: 0.4174  last_time: 0.4326  data_time: 0.0281  last_data_time: 0.0447   lr: 0.02  max_mem: 11505M
[32m[11/10 16:19:19 d2.utils.events]: [0m eta: 0:37:21  iter: 3159  total_loss: 0.4829  loss_cls: 0.1319  loss_box_reg: 0.3163  loss_rpn_cls: 0.005598  loss_rpn_loc: 0.007332    time: 0.4174  last_time: 0.4605  data_time: 0.0320  last_data_time: 0.0276   lr: 0.02  max_mem: 11505M
[32m[11/10 16:19:27 d2.utils.events]: [0m eta: 0:37:12  iter: 3179  total_loss: 0.4953  loss_cls: 0.1313  loss_box_reg: 0.3344  loss_rpn_cls: 0.007529  loss_rpn_loc: 0.008336    time: 0.4174  last_time: 0.3545  data_time: 0.0275  last_data_time: 0.0061   lr: 0.02  max_mem: 11505M
[32m[11/10 16:19:35 d2.utils.events]: [0m eta: 0:36:59  iter: 3199  total_loss: 0.5069  loss_cls: 0.1424  loss_box_reg: 0.3305  loss_rpn_cls: 0.01145  loss_rpn_loc: 0.009407    time: 0.4173  last_time: 0.4417  data_time: 0.0347  last_data_time: 0.0291   lr: 0.02  max_mem: 11505M
[32m[11/10 16:19:44 d2.utils.events]: [0m eta: 0:36:55  iter: 3219  total_loss: 0.4691  loss_cls: 0.1281  loss_box_reg: 0.3281  loss_rpn_cls: 0.007416  loss_rpn_loc: 0.009634    time: 0.4173  last_time: 0.3936  data_time: 0.0355  last_data_time: 0.0138   lr: 0.02  max_mem: 11505M
[32m[11/10 16:19:52 d2.utils.events]: [0m eta: 0:36:47  iter: 3239  total_loss: 0.4778  loss_cls: 0.1398  loss_box_reg: 0.2933  loss_rpn_cls: 0.009941  loss_rpn_loc: 0.01053    time: 0.4173  last_time: 0.3894  data_time: 0.0339  last_data_time: 0.0049   lr: 0.02  max_mem: 11505M
[32m[11/10 16:20:01 d2.utils.events]: [0m eta: 0:36:39  iter: 3259  total_loss: 0.5008  loss_cls: 0.1565  loss_box_reg: 0.3039  loss_rpn_cls: 0.00963  loss_rpn_loc: 0.008928    time: 0.4173  last_time: 0.4061  data_time: 0.0350  last_data_time: 0.0192   lr: 0.02  max_mem: 11505M
[32m[11/10 16:20:09 d2.utils.events]: [0m eta: 0:36:30  iter: 3279  total_loss: 0.4369  loss_cls: 0.1229  loss_box_reg: 0.2935  loss_rpn_cls: 0.009527  loss_rpn_loc: 0.009716    time: 0.4173  last_time: 0.3200  data_time: 0.0302  last_data_time: 0.0059   lr: 0.02  max_mem: 11505M
[32m[11/10 16:20:17 d2.utils.events]: [0m eta: 0:36:22  iter: 3299  total_loss: 0.4666  loss_cls: 0.1371  loss_box_reg: 0.3011  loss_rpn_cls: 0.008114  loss_rpn_loc: 0.00819    time: 0.4174  last_time: 0.3480  data_time: 0.0318  last_data_time: 0.0039   lr: 0.02  max_mem: 11505M
[32m[11/10 16:20:26 d2.utils.events]: [0m eta: 0:36:14  iter: 3319  total_loss: 0.414  loss_cls: 0.1179  loss_box_reg: 0.2822  loss_rpn_cls: 0.006068  loss_rpn_loc: 0.009671    time: 0.4174  last_time: 0.4371  data_time: 0.0379  last_data_time: 0.0311   lr: 0.02  max_mem: 11505M
[32m[11/10 16:20:34 d2.utils.events]: [0m eta: 0:36:05  iter: 3339  total_loss: 0.4416  loss_cls: 0.1361  loss_box_reg: 0.2974  loss_rpn_cls: 0.007837  loss_rpn_loc: 0.008141    time: 0.4174  last_time: 0.4411  data_time: 0.0387  last_data_time: 0.0477   lr: 0.02  max_mem: 11505M
[32m[11/10 16:20:42 d2.utils.events]: [0m eta: 0:35:56  iter: 3359  total_loss: 0.4149  loss_cls: 0.1193  loss_box_reg: 0.2962  loss_rpn_cls: 0.005688  loss_rpn_loc: 0.007274    time: 0.4174  last_time: 0.3665  data_time: 0.0323  last_data_time: 0.0239   lr: 0.02  max_mem: 11505M
[32m[11/10 16:20:51 d2.utils.events]: [0m eta: 0:35:46  iter: 3379  total_loss: 0.4109  loss_cls: 0.1114  loss_box_reg: 0.2788  loss_rpn_cls: 0.006242  loss_rpn_loc: 0.007857    time: 0.4173  last_time: 0.3679  data_time: 0.0320  last_data_time: 0.0293   lr: 0.02  max_mem: 11505M
[32m[11/10 16:20:59 d2.utils.events]: [0m eta: 0:35:32  iter: 3399  total_loss: 0.4417  loss_cls: 0.1018  loss_box_reg: 0.3167  loss_rpn_cls: 0.006632  loss_rpn_loc: 0.009531    time: 0.4173  last_time: 0.4688  data_time: 0.0327  last_data_time: 0.0437   lr: 0.02  max_mem: 11505M
[32m[11/10 16:21:07 d2.utils.events]: [0m eta: 0:35:24  iter: 3419  total_loss: 0.4054  loss_cls: 0.1138  loss_box_reg: 0.2677  loss_rpn_cls: 0.005946  loss_rpn_loc: 0.006167    time: 0.4173  last_time: 0.4342  data_time: 0.0356  last_data_time: 0.0079   lr: 0.02  max_mem: 11505M
[32m[11/10 16:21:16 d2.utils.events]: [0m eta: 0:35:16  iter: 3439  total_loss: 0.4936  loss_cls: 0.1433  loss_box_reg: 0.3174  loss_rpn_cls: 0.007713  loss_rpn_loc: 0.009645    time: 0.4173  last_time: 0.3930  data_time: 0.0324  last_data_time: 0.0057   lr: 0.02  max_mem: 11505M
[32m[11/10 16:21:24 d2.utils.events]: [0m eta: 0:35:08  iter: 3459  total_loss: 0.4564  loss_cls: 0.1243  loss_box_reg: 0.3071  loss_rpn_cls: 0.009236  loss_rpn_loc: 0.008168    time: 0.4174  last_time: 0.4179  data_time: 0.0354  last_data_time: 0.0311   lr: 0.02  max_mem: 11505M
[32m[11/10 16:21:32 d2.utils.events]: [0m eta: 0:34:59  iter: 3479  total_loss: 0.4778  loss_cls: 0.1243  loss_box_reg: 0.3242  loss_rpn_cls: 0.008227  loss_rpn_loc: 0.008942    time: 0.4173  last_time: 0.3808  data_time: 0.0323  last_data_time: 0.0085   lr: 0.02  max_mem: 11505M
[32m[11/10 16:21:41 d2.utils.events]: [0m eta: 0:34:53  iter: 3499  total_loss: 0.4383  loss_cls: 0.1236  loss_box_reg: 0.2925  loss_rpn_cls: 0.006551  loss_rpn_loc: 0.009537    time: 0.4174  last_time: 0.4304  data_time: 0.0343  last_data_time: 0.0090   lr: 0.02  max_mem: 11505M
[32m[11/10 16:21:49 d2.utils.events]: [0m eta: 0:34:50  iter: 3519  total_loss: 0.4114  loss_cls: 0.1274  loss_box_reg: 0.258  loss_rpn_cls: 0.007871  loss_rpn_loc: 0.007689    time: 0.4175  last_time: 0.3979  data_time: 0.0315  last_data_time: 0.0203   lr: 0.02  max_mem: 11505M
[32m[11/10 16:21:58 d2.utils.events]: [0m eta: 0:34:41  iter: 3539  total_loss: 0.4202  loss_cls: 0.1142  loss_box_reg: 0.2868  loss_rpn_cls: 0.008111  loss_rpn_loc: 0.008935    time: 0.4174  last_time: 0.4063  data_time: 0.0317  last_data_time: 0.0117   lr: 0.02  max_mem: 11505M
[32m[11/10 16:22:06 d2.utils.events]: [0m eta: 0:34:33  iter: 3559  total_loss: 0.4473  loss_cls: 0.124  loss_box_reg: 0.3067  loss_rpn_cls: 0.01033  loss_rpn_loc: 0.008476    time: 0.4174  last_time: 0.3890  data_time: 0.0315  last_data_time: 0.0261   lr: 0.02  max_mem: 11505M
[32m[11/10 16:22:14 d2.utils.events]: [0m eta: 0:34:24  iter: 3579  total_loss: 0.448  loss_cls: 0.119  loss_box_reg: 0.315  loss_rpn_cls: 0.006948  loss_rpn_loc: 0.007955    time: 0.4174  last_time: 0.3958  data_time: 0.0356  last_data_time: 0.0340   lr: 0.02  max_mem: 11505M
[32m[11/10 16:22:23 d2.utils.events]: [0m eta: 0:34:15  iter: 3599  total_loss: 0.5055  loss_cls: 0.1493  loss_box_reg: 0.3068  loss_rpn_cls: 0.007812  loss_rpn_loc: 0.009238    time: 0.4174  last_time: 0.4276  data_time: 0.0354  last_data_time: 0.0491   lr: 0.02  max_mem: 11505M
[32m[11/10 16:22:31 d2.utils.events]: [0m eta: 0:34:06  iter: 3619  total_loss: 0.5068  loss_cls: 0.1439  loss_box_reg: 0.3391  loss_rpn_cls: 0.008119  loss_rpn_loc: 0.009172    time: 0.4174  last_time: 0.4531  data_time: 0.0342  last_data_time: 0.0326   lr: 0.02  max_mem: 11505M
[32m[11/10 16:22:39 d2.utils.events]: [0m eta: 0:33:56  iter: 3639  total_loss: 0.4853  loss_cls: 0.1324  loss_box_reg: 0.3342  loss_rpn_cls: 0.008574  loss_rpn_loc: 0.01048    time: 0.4174  last_time: 0.3673  data_time: 0.0360  last_data_time: 0.0159   lr: 0.02  max_mem: 11505M
[32m[11/10 16:22:48 d2.utils.events]: [0m eta: 0:33:46  iter: 3659  total_loss: 0.5017  loss_cls: 0.138  loss_box_reg: 0.3443  loss_rpn_cls: 0.009711  loss_rpn_loc: 0.008396    time: 0.4173  last_time: 0.4043  data_time: 0.0320  last_data_time: 0.0509   lr: 0.02  max_mem: 11505M
[32m[11/10 16:22:56 d2.utils.events]: [0m eta: 0:33:36  iter: 3679  total_loss: 0.4384  loss_cls: 0.1201  loss_box_reg: 0.3022  loss_rpn_cls: 0.007434  loss_rpn_loc: 0.008775    time: 0.4173  last_time: 0.4270  data_time: 0.0320  last_data_time: 0.0388   lr: 0.02  max_mem: 11505M
[32m[11/10 16:23:04 d2.utils.events]: [0m eta: 0:33:31  iter: 3699  total_loss: 0.4156  loss_cls: 0.1069  loss_box_reg: 0.2673  loss_rpn_cls: 0.007326  loss_rpn_loc: 0.009639    time: 0.4174  last_time: 0.3643  data_time: 0.0322  last_data_time: 0.0183   lr: 0.02  max_mem: 11505M
[32m[11/10 16:23:13 d2.utils.events]: [0m eta: 0:33:23  iter: 3719  total_loss: 0.4296  loss_cls: 0.1145  loss_box_reg: 0.2891  loss_rpn_cls: 0.007417  loss_rpn_loc: 0.008491    time: 0.4174  last_time: 0.3629  data_time: 0.0376  last_data_time: 0.0026   lr: 0.02  max_mem: 11505M
[32m[11/10 16:23:21 d2.utils.events]: [0m eta: 0:33:13  iter: 3739  total_loss: 0.4482  loss_cls: 0.1137  loss_box_reg: 0.3137  loss_rpn_cls: 0.007247  loss_rpn_loc: 0.00755    time: 0.4174  last_time: 0.4497  data_time: 0.0351  last_data_time: 0.0274   lr: 0.02  max_mem: 11505M
[32m[11/10 16:23:30 d2.utils.events]: [0m eta: 0:33:04  iter: 3759  total_loss: 0.4772  loss_cls: 0.1286  loss_box_reg: 0.3472  loss_rpn_cls: 0.006883  loss_rpn_loc: 0.00787    time: 0.4175  last_time: 0.3974  data_time: 0.0327  last_data_time: 0.0309   lr: 0.02  max_mem: 11505M
[32m[11/10 16:23:38 d2.utils.events]: [0m eta: 0:32:57  iter: 3779  total_loss: 0.5062  loss_cls: 0.1362  loss_box_reg: 0.3552  loss_rpn_cls: 0.008421  loss_rpn_loc: 0.007343    time: 0.4175  last_time: 0.4519  data_time: 0.0320  last_data_time: 0.0538   lr: 0.02  max_mem: 11505M
[32m[11/10 16:23:47 d2.utils.events]: [0m eta: 0:32:49  iter: 3799  total_loss: 0.4532  loss_cls: 0.1295  loss_box_reg: 0.3015  loss_rpn_cls: 0.008765  loss_rpn_loc: 0.00956    time: 0.4175  last_time: 0.4199  data_time: 0.0334  last_data_time: 0.0086   lr: 0.02  max_mem: 11505M
[32m[11/10 16:23:55 d2.utils.events]: [0m eta: 0:32:42  iter: 3819  total_loss: 0.4414  loss_cls: 0.1225  loss_box_reg: 0.2976  loss_rpn_cls: 0.006676  loss_rpn_loc: 0.00806    time: 0.4176  last_time: 0.4161  data_time: 0.0345  last_data_time: 0.0070   lr: 0.02  max_mem: 11505M
[32m[11/10 16:24:04 d2.utils.events]: [0m eta: 0:32:35  iter: 3839  total_loss: 0.513  loss_cls: 0.1428  loss_box_reg: 0.3307  loss_rpn_cls: 0.007004  loss_rpn_loc: 0.01117    time: 0.4176  last_time: 0.4184  data_time: 0.0365  last_data_time: 0.0371   lr: 0.02  max_mem: 11505M
[32m[11/10 16:24:12 d2.utils.events]: [0m eta: 0:32:26  iter: 3859  total_loss: 0.4176  loss_cls: 0.1085  loss_box_reg: 0.2713  loss_rpn_cls: 0.00605  loss_rpn_loc: 0.008311    time: 0.4176  last_time: 0.3785  data_time: 0.0345  last_data_time: 0.0354   lr: 0.02  max_mem: 11505M
[32m[11/10 16:24:21 d2.utils.events]: [0m eta: 0:32:17  iter: 3879  total_loss: 0.4292  loss_cls: 0.107  loss_box_reg: 0.3026  loss_rpn_cls: 0.006689  loss_rpn_loc: 0.007272    time: 0.4177  last_time: 0.4209  data_time: 0.0304  last_data_time: 0.0354   lr: 0.02  max_mem: 11505M
[32m[11/10 16:24:29 d2.utils.events]: [0m eta: 0:32:08  iter: 3899  total_loss: 0.3994  loss_cls: 0.08894  loss_box_reg: 0.2758  loss_rpn_cls: 0.007933  loss_rpn_loc: 0.007881    time: 0.4177  last_time: 0.4620  data_time: 0.0275  last_data_time: 0.0404   lr: 0.02  max_mem: 11505M
[32m[11/10 16:24:37 d2.utils.events]: [0m eta: 0:31:57  iter: 3919  total_loss: 0.4081  loss_cls: 0.1139  loss_box_reg: 0.2798  loss_rpn_cls: 0.008622  loss_rpn_loc: 0.006354    time: 0.4177  last_time: 0.4661  data_time: 0.0303  last_data_time: 0.0522   lr: 0.02  max_mem: 11505M
[32m[11/10 16:24:46 d2.utils.events]: [0m eta: 0:31:49  iter: 3939  total_loss: 0.4222  loss_cls: 0.1124  loss_box_reg: 0.2953  loss_rpn_cls: 0.008483  loss_rpn_loc: 0.009715    time: 0.4176  last_time: 0.4507  data_time: 0.0313  last_data_time: 0.0689   lr: 0.02  max_mem: 11505M
[32m[11/10 16:24:54 d2.utils.events]: [0m eta: 0:31:41  iter: 3959  total_loss: 0.4396  loss_cls: 0.1318  loss_box_reg: 0.274  loss_rpn_cls: 0.0105  loss_rpn_loc: 0.01132    time: 0.4177  last_time: 0.4243  data_time: 0.0325  last_data_time: 0.0174   lr: 0.02  max_mem: 11505M
[32m[11/10 16:25:03 d2.utils.events]: [0m eta: 0:31:32  iter: 3979  total_loss: 0.4373  loss_cls: 0.1138  loss_box_reg: 0.2808  loss_rpn_cls: 0.006495  loss_rpn_loc: 0.008042    time: 0.4177  last_time: 0.3705  data_time: 0.0308  last_data_time: 0.0140   lr: 0.02  max_mem: 11505M
[32m[11/10 16:25:11 d2.utils.events]: [0m eta: 0:31:25  iter: 3999  total_loss: 0.4073  loss_cls: 0.1124  loss_box_reg: 0.2771  loss_rpn_cls: 0.006699  loss_rpn_loc: 0.007556    time: 0.4177  last_time: 0.4328  data_time: 0.0379  last_data_time: 0.0531   lr: 0.02  max_mem: 11505M
[32m[11/10 16:25:19 d2.utils.events]: [0m eta: 0:31:17  iter: 4019  total_loss: 0.4481  loss_cls: 0.1188  loss_box_reg: 0.3261  loss_rpn_cls: 0.005687  loss_rpn_loc: 0.006259    time: 0.4177  last_time: 0.4340  data_time: 0.0304  last_data_time: 0.0417   lr: 0.02  max_mem: 11505M
[32m[11/10 16:25:28 d2.utils.events]: [0m eta: 0:31:09  iter: 4039  total_loss: 0.4846  loss_cls: 0.1409  loss_box_reg: 0.3392  loss_rpn_cls: 0.005648  loss_rpn_loc: 0.009911    time: 0.4177  last_time: 0.4534  data_time: 0.0323  last_data_time: 0.0455   lr: 0.02  max_mem: 11505M
[32m[11/10 16:25:36 d2.utils.events]: [0m eta: 0:30:59  iter: 4059  total_loss: 0.4007  loss_cls: 0.1048  loss_box_reg: 0.2706  loss_rpn_cls: 0.005675  loss_rpn_loc: 0.007838    time: 0.4177  last_time: 0.4539  data_time: 0.0375  last_data_time: 0.0602   lr: 0.02  max_mem: 11505M
[32m[11/10 16:25:45 d2.utils.events]: [0m eta: 0:30:51  iter: 4079  total_loss: 0.4152  loss_cls: 0.121  loss_box_reg: 0.278  loss_rpn_cls: 0.008349  loss_rpn_loc: 0.006622    time: 0.4177  last_time: 0.4038  data_time: 0.0301  last_data_time: 0.0345   lr: 0.02  max_mem: 11505M
[32m[11/10 16:25:53 d2.utils.events]: [0m eta: 0:30:43  iter: 4099  total_loss: 0.4422  loss_cls: 0.1159  loss_box_reg: 0.307  loss_rpn_cls: 0.007101  loss_rpn_loc: 0.008396    time: 0.4178  last_time: 0.4329  data_time: 0.0389  last_data_time: 0.0370   lr: 0.02  max_mem: 11505M
[32m[11/10 16:26:02 d2.utils.events]: [0m eta: 0:30:35  iter: 4119  total_loss: 0.4336  loss_cls: 0.1129  loss_box_reg: 0.2967  loss_rpn_cls: 0.009296  loss_rpn_loc: 0.009473    time: 0.4178  last_time: 0.4706  data_time: 0.0364  last_data_time: 0.0548   lr: 0.02  max_mem: 11505M
[32m[11/10 16:26:10 d2.utils.events]: [0m eta: 0:30:26  iter: 4139  total_loss: 0.4363  loss_cls: 0.1208  loss_box_reg: 0.2877  loss_rpn_cls: 0.007135  loss_rpn_loc: 0.009033    time: 0.4177  last_time: 0.4276  data_time: 0.0281  last_data_time: 0.0342   lr: 0.02  max_mem: 11505M
[32m[11/10 16:26:18 d2.utils.events]: [0m eta: 0:30:18  iter: 4159  total_loss: 0.4086  loss_cls: 0.1107  loss_box_reg: 0.2812  loss_rpn_cls: 0.006592  loss_rpn_loc: 0.007613    time: 0.4178  last_time: 0.4013  data_time: 0.0305  last_data_time: 0.0139   lr: 0.02  max_mem: 11505M
[32m[11/10 16:26:27 d2.utils.events]: [0m eta: 0:30:12  iter: 4179  total_loss: 0.4901  loss_cls: 0.1346  loss_box_reg: 0.3112  loss_rpn_cls: 0.009314  loss_rpn_loc: 0.009177    time: 0.4178  last_time: 0.3918  data_time: 0.0340  last_data_time: 0.0190   lr: 0.02  max_mem: 11505M
[32m[11/10 16:26:35 d2.utils.events]: [0m eta: 0:30:07  iter: 4199  total_loss: 0.4583  loss_cls: 0.1364  loss_box_reg: 0.3015  loss_rpn_cls: 0.006312  loss_rpn_loc: 0.008712    time: 0.4179  last_time: 0.4263  data_time: 0.0344  last_data_time: 0.0055   lr: 0.02  max_mem: 11505M
[32m[11/10 16:26:44 d2.utils.events]: [0m eta: 0:29:56  iter: 4219  total_loss: 0.4187  loss_cls: 0.1049  loss_box_reg: 0.2881  loss_rpn_cls: 0.005941  loss_rpn_loc: 0.008405    time: 0.4179  last_time: 0.4014  data_time: 0.0355  last_data_time: 0.0119   lr: 0.02  max_mem: 11505M
[32m[11/10 16:26:52 d2.utils.events]: [0m eta: 0:29:46  iter: 4239  total_loss: 0.4637  loss_cls: 0.1145  loss_box_reg: 0.3195  loss_rpn_cls: 0.007042  loss_rpn_loc: 0.009152    time: 0.4178  last_time: 0.4340  data_time: 0.0348  last_data_time: 0.0497   lr: 0.02  max_mem: 11505M
[32m[11/10 16:27:00 d2.utils.events]: [0m eta: 0:29:37  iter: 4259  total_loss: 0.4227  loss_cls: 0.1143  loss_box_reg: 0.2753  loss_rpn_cls: 0.007538  loss_rpn_loc: 0.007806    time: 0.4178  last_time: 0.4046  data_time: 0.0346  last_data_time: 0.0402   lr: 0.02  max_mem: 11505M
[32m[11/10 16:27:09 d2.utils.events]: [0m eta: 0:29:28  iter: 4279  total_loss: 0.4328  loss_cls: 0.1139  loss_box_reg: 0.2975  loss_rpn_cls: 0.008283  loss_rpn_loc: 0.008842    time: 0.4179  last_time: 0.3844  data_time: 0.0303  last_data_time: 0.0108   lr: 0.02  max_mem: 11505M
[32m[11/10 16:27:17 d2.utils.events]: [0m eta: 0:29:20  iter: 4299  total_loss: 0.4346  loss_cls: 0.1247  loss_box_reg: 0.2927  loss_rpn_cls: 0.008899  loss_rpn_loc: 0.006432    time: 0.4179  last_time: 0.4300  data_time: 0.0304  last_data_time: 0.0193   lr: 0.02  max_mem: 11505M
[32m[11/10 16:27:25 d2.utils.events]: [0m eta: 0:29:12  iter: 4319  total_loss: 0.4093  loss_cls: 0.117  loss_box_reg: 0.273  loss_rpn_cls: 0.007663  loss_rpn_loc: 0.008245    time: 0.4179  last_time: 0.4556  data_time: 0.0329  last_data_time: 0.0323   lr: 0.02  max_mem: 11505M
[32m[11/10 16:27:34 d2.utils.events]: [0m eta: 0:29:04  iter: 4339  total_loss: 0.481  loss_cls: 0.1339  loss_box_reg: 0.3172  loss_rpn_cls: 0.008035  loss_rpn_loc: 0.006789    time: 0.4179  last_time: 0.4046  data_time: 0.0345  last_data_time: 0.0307   lr: 0.02  max_mem: 11505M
[32m[11/10 16:27:42 d2.utils.events]: [0m eta: 0:28:56  iter: 4359  total_loss: 0.4031  loss_cls: 0.1037  loss_box_reg: 0.2961  loss_rpn_cls: 0.005154  loss_rpn_loc: 0.006314    time: 0.4178  last_time: 0.4296  data_time: 0.0350  last_data_time: 0.0376   lr: 0.02  max_mem: 11505M
[32m[11/10 16:27:51 d2.utils.events]: [0m eta: 0:28:49  iter: 4379  total_loss: 0.4281  loss_cls: 0.1137  loss_box_reg: 0.2843  loss_rpn_cls: 0.005015  loss_rpn_loc: 0.007679    time: 0.4180  last_time: 0.3947  data_time: 0.0341  last_data_time: 0.0279   lr: 0.02  max_mem: 11505M
[32m[11/10 16:27:59 d2.utils.events]: [0m eta: 0:28:41  iter: 4399  total_loss: 0.4161  loss_cls: 0.1093  loss_box_reg: 0.2847  loss_rpn_cls: 0.006994  loss_rpn_loc: 0.006889    time: 0.4179  last_time: 0.4238  data_time: 0.0340  last_data_time: 0.0220   lr: 0.02  max_mem: 11505M
[32m[11/10 16:28:08 d2.utils.events]: [0m eta: 0:28:32  iter: 4419  total_loss: 0.4383  loss_cls: 0.1352  loss_box_reg: 0.2828  loss_rpn_cls: 0.007765  loss_rpn_loc: 0.008091    time: 0.4180  last_time: 0.3895  data_time: 0.0339  last_data_time: 0.0185   lr: 0.02  max_mem: 11505M
[32m[11/10 16:28:16 d2.utils.events]: [0m eta: 0:28:23  iter: 4439  total_loss: 0.4325  loss_cls: 0.116  loss_box_reg: 0.2893  loss_rpn_cls: 0.006229  loss_rpn_loc: 0.009227    time: 0.4180  last_time: 0.4792  data_time: 0.0354  last_data_time: 0.0516   lr: 0.02  max_mem: 11505M
[32m[11/10 16:28:25 d2.utils.events]: [0m eta: 0:28:14  iter: 4459  total_loss: 0.4483  loss_cls: 0.1246  loss_box_reg: 0.2629  loss_rpn_cls: 0.007582  loss_rpn_loc: 0.007307    time: 0.4180  last_time: 0.3960  data_time: 0.0312  last_data_time: 0.0053   lr: 0.02  max_mem: 11505M
[32m[11/10 16:28:33 d2.utils.events]: [0m eta: 0:28:06  iter: 4479  total_loss: 0.4416  loss_cls: 0.1093  loss_box_reg: 0.2885  loss_rpn_cls: 0.007723  loss_rpn_loc: 0.006894    time: 0.4180  last_time: 0.4454  data_time: 0.0325  last_data_time: 0.0468   lr: 0.02  max_mem: 11505M
[32m[11/10 16:28:42 d2.utils.events]: [0m eta: 0:27:57  iter: 4499  total_loss: 0.4482  loss_cls: 0.1223  loss_box_reg: 0.3131  loss_rpn_cls: 0.005805  loss_rpn_loc: 0.008424    time: 0.4181  last_time: 0.4634  data_time: 0.0319  last_data_time: 0.0366   lr: 0.02  max_mem: 11505M
[32m[11/10 16:28:50 d2.utils.events]: [0m eta: 0:27:48  iter: 4519  total_loss: 0.4316  loss_cls: 0.1141  loss_box_reg: 0.2979  loss_rpn_cls: 0.006286  loss_rpn_loc: 0.008808    time: 0.4181  last_time: 0.3965  data_time: 0.0284  last_data_time: 0.0306   lr: 0.02  max_mem: 11505M
[32m[11/10 16:28:58 d2.utils.events]: [0m eta: 0:27:40  iter: 4539  total_loss: 0.4139  loss_cls: 0.09846  loss_box_reg: 0.2938  loss_rpn_cls: 0.006976  loss_rpn_loc: 0.008655    time: 0.4181  last_time: 0.4109  data_time: 0.0290  last_data_time: 0.0067   lr: 0.02  max_mem: 11505M
[32m[11/10 16:29:07 d2.utils.events]: [0m eta: 0:27:32  iter: 4559  total_loss: 0.3911  loss_cls: 0.1006  loss_box_reg: 0.2676  loss_rpn_cls: 0.009495  loss_rpn_loc: 0.007334    time: 0.4181  last_time: 0.3887  data_time: 0.0320  last_data_time: 0.0278   lr: 0.02  max_mem: 11505M
[32m[11/10 16:29:15 d2.utils.events]: [0m eta: 0:27:24  iter: 4579  total_loss: 0.4253  loss_cls: 0.1142  loss_box_reg: 0.2815  loss_rpn_cls: 0.005371  loss_rpn_loc: 0.008731    time: 0.4181  last_time: 0.4861  data_time: 0.0361  last_data_time: 0.0977   lr: 0.02  max_mem: 11505M
[32m[11/10 16:29:23 d2.utils.events]: [0m eta: 0:27:15  iter: 4599  total_loss: 0.4175  loss_cls: 0.1029  loss_box_reg: 0.2893  loss_rpn_cls: 0.007397  loss_rpn_loc: 0.006408    time: 0.4180  last_time: 0.4030  data_time: 0.0309  last_data_time: 0.0174   lr: 0.02  max_mem: 11505M
[32m[11/10 16:29:32 d2.utils.events]: [0m eta: 0:27:07  iter: 4619  total_loss: 0.4319  loss_cls: 0.1145  loss_box_reg: 0.2987  loss_rpn_cls: 0.00874  loss_rpn_loc: 0.01051    time: 0.4181  last_time: 0.4402  data_time: 0.0374  last_data_time: 0.0217   lr: 0.02  max_mem: 11505M
[32m[11/10 16:29:40 d2.utils.events]: [0m eta: 0:26:59  iter: 4639  total_loss: 0.4176  loss_cls: 0.1126  loss_box_reg: 0.2942  loss_rpn_cls: 0.007595  loss_rpn_loc: 0.009265    time: 0.4180  last_time: 0.4317  data_time: 0.0337  last_data_time: 0.0499   lr: 0.02  max_mem: 11505M
[32m[11/10 16:29:48 d2.utils.events]: [0m eta: 0:26:50  iter: 4659  total_loss: 0.439  loss_cls: 0.1195  loss_box_reg: 0.3048  loss_rpn_cls: 0.005956  loss_rpn_loc: 0.006918    time: 0.4180  last_time: 0.4111  data_time: 0.0313  last_data_time: 0.0604   lr: 0.02  max_mem: 11505M
[32m[11/10 16:29:57 d2.utils.events]: [0m eta: 0:26:42  iter: 4679  total_loss: 0.4059  loss_cls: 0.1015  loss_box_reg: 0.2729  loss_rpn_cls: 0.01095  loss_rpn_loc: 0.008759    time: 0.4181  last_time: 0.4190  data_time: 0.0338  last_data_time: 0.0188   lr: 0.02  max_mem: 11505M
[32m[11/10 16:30:05 d2.utils.events]: [0m eta: 0:26:33  iter: 4699  total_loss: 0.44  loss_cls: 0.1308  loss_box_reg: 0.2963  loss_rpn_cls: 0.007761  loss_rpn_loc: 0.009916    time: 0.4181  last_time: 0.4540  data_time: 0.0349  last_data_time: 0.0298   lr: 0.02  max_mem: 11505M
[32m[11/10 16:30:14 d2.utils.events]: [0m eta: 0:26:27  iter: 4719  total_loss: 0.374  loss_cls: 0.0995  loss_box_reg: 0.2657  loss_rpn_cls: 0.007579  loss_rpn_loc: 0.009502    time: 0.4181  last_time: 0.4182  data_time: 0.0366  last_data_time: 0.0093   lr: 0.02  max_mem: 11505M
[32m[11/10 16:30:22 d2.utils.events]: [0m eta: 0:26:18  iter: 4739  total_loss: 0.4251  loss_cls: 0.1178  loss_box_reg: 0.2862  loss_rpn_cls: 0.006521  loss_rpn_loc: 0.006876    time: 0.4182  last_time: 0.4021  data_time: 0.0324  last_data_time: 0.0039   lr: 0.02  max_mem: 11505M
[32m[11/10 16:30:31 d2.utils.events]: [0m eta: 0:26:09  iter: 4759  total_loss: 0.4116  loss_cls: 0.1233  loss_box_reg: 0.2763  loss_rpn_cls: 0.00564  loss_rpn_loc: 0.006979    time: 0.4181  last_time: 0.3987  data_time: 0.0316  last_data_time: 0.0586   lr: 0.02  max_mem: 11505M
[32m[11/10 16:30:39 d2.utils.events]: [0m eta: 0:26:01  iter: 4779  total_loss: 0.3941  loss_cls: 0.1109  loss_box_reg: 0.2691  loss_rpn_cls: 0.007273  loss_rpn_loc: 0.01167    time: 0.4182  last_time: 0.5017  data_time: 0.0386  last_data_time: 0.1224   lr: 0.02  max_mem: 11505M
[32m[11/10 16:30:48 d2.utils.events]: [0m eta: 0:25:52  iter: 4799  total_loss: 0.4168  loss_cls: 0.1084  loss_box_reg: 0.287  loss_rpn_cls: 0.007154  loss_rpn_loc: 0.01014    time: 0.4181  last_time: 0.3431  data_time: 0.0298  last_data_time: 0.0042   lr: 0.02  max_mem: 11505M
[32m[11/10 16:30:56 d2.utils.events]: [0m eta: 0:25:45  iter: 4819  total_loss: 0.4195  loss_cls: 0.1068  loss_box_reg: 0.2971  loss_rpn_cls: 0.007051  loss_rpn_loc: 0.007593    time: 0.4183  last_time: 0.4552  data_time: 0.0468  last_data_time: 0.0849   lr: 0.02  max_mem: 11505M
[32m[11/10 16:31:05 d2.utils.events]: [0m eta: 0:25:35  iter: 4839  total_loss: 0.4498  loss_cls: 0.1226  loss_box_reg: 0.3049  loss_rpn_cls: 0.006383  loss_rpn_loc: 0.009576    time: 0.4183  last_time: 0.4032  data_time: 0.0307  last_data_time: 0.0112   lr: 0.02  max_mem: 11505M
[32m[11/10 16:31:13 d2.utils.events]: [0m eta: 0:25:28  iter: 4859  total_loss: 0.4166  loss_cls: 0.126  loss_box_reg: 0.269  loss_rpn_cls: 0.008392  loss_rpn_loc: 0.007917    time: 0.4183  last_time: 0.4555  data_time: 0.0358  last_data_time: 0.0572   lr: 0.02  max_mem: 11505M
[32m[11/10 16:31:22 d2.utils.events]: [0m eta: 0:25:20  iter: 4879  total_loss: 0.4101  loss_cls: 0.1135  loss_box_reg: 0.2746  loss_rpn_cls: 0.006613  loss_rpn_loc: 0.007199    time: 0.4184  last_time: 0.4252  data_time: 0.0386  last_data_time: 0.0584   lr: 0.02  max_mem: 11505M
[32m[11/10 16:31:31 d2.utils.events]: [0m eta: 0:25:12  iter: 4899  total_loss: 0.4279  loss_cls: 0.109  loss_box_reg: 0.2893  loss_rpn_cls: 0.00608  loss_rpn_loc: 0.007441    time: 0.4184  last_time: 0.4040  data_time: 0.0397  last_data_time: 0.0513   lr: 0.02  max_mem: 11505M
[32m[11/10 16:31:39 d2.utils.events]: [0m eta: 0:25:06  iter: 4919  total_loss: 0.3795  loss_cls: 0.1004  loss_box_reg: 0.2571  loss_rpn_cls: 0.007741  loss_rpn_loc: 0.007853    time: 0.4184  last_time: 0.4982  data_time: 0.0296  last_data_time: 0.0473   lr: 0.02  max_mem: 11505M
[32m[11/10 16:31:48 d2.utils.events]: [0m eta: 0:24:59  iter: 4939  total_loss: 0.4151  loss_cls: 0.1036  loss_box_reg: 0.296  loss_rpn_cls: 0.0046  loss_rpn_loc: 0.007575    time: 0.4185  last_time: 0.4222  data_time: 0.0351  last_data_time: 0.0169   lr: 0.02  max_mem: 11505M
[32m[11/10 16:31:56 d2.utils.events]: [0m eta: 0:24:50  iter: 4959  total_loss: 0.3833  loss_cls: 0.1115  loss_box_reg: 0.2635  loss_rpn_cls: 0.006527  loss_rpn_loc: 0.006089    time: 0.4185  last_time: 0.3584  data_time: 0.0287  last_data_time: 0.0068   lr: 0.02  max_mem: 11505M
[32m[11/10 16:32:04 d2.utils.events]: [0m eta: 0:24:41  iter: 4979  total_loss: 0.3709  loss_cls: 0.1157  loss_box_reg: 0.246  loss_rpn_cls: 0.006475  loss_rpn_loc: 0.006335    time: 0.4185  last_time: 0.4210  data_time: 0.0353  last_data_time: 0.0577   lr: 0.02  max_mem: 11505M
[32m[11/10 16:32:14 d2.utils.events]: [0m eta: 0:24:33  iter: 4999  total_loss: 0.4276  loss_cls: 0.1214  loss_box_reg: 0.2914  loss_rpn_cls: 0.00741  loss_rpn_loc: 0.007899    time: 0.4185  last_time: 0.4433  data_time: 0.0353  last_data_time: 0.0303   lr: 0.02  max_mem: 11505M
[32m[11/10 16:32:22 d2.utils.events]: [0m eta: 0:24:25  iter: 5019  total_loss: 0.3805  loss_cls: 0.09764  loss_box_reg: 0.2796  loss_rpn_cls: 0.004977  loss_rpn_loc: 0.006147    time: 0.4185  last_time: 0.4711  data_time: 0.0324  last_data_time: 0.0539   lr: 0.02  max_mem: 11505M
[32m[11/10 16:32:31 d2.utils.events]: [0m eta: 0:24:16  iter: 5039  total_loss: 0.4089  loss_cls: 0.1025  loss_box_reg: 0.2921  loss_rpn_cls: 0.008384  loss_rpn_loc: 0.009268    time: 0.4185  last_time: 0.4000  data_time: 0.0338  last_data_time: 0.0124   lr: 0.02  max_mem: 11505M
[32m[11/10 16:32:39 d2.utils.events]: [0m eta: 0:24:08  iter: 5059  total_loss: 0.3886  loss_cls: 0.1017  loss_box_reg: 0.2724  loss_rpn_cls: 0.00549  loss_rpn_loc: 0.007174    time: 0.4186  last_time: 0.4500  data_time: 0.0407  last_data_time: 0.0876   lr: 0.02  max_mem: 11505M
[32m[11/10 16:32:48 d2.utils.events]: [0m eta: 0:24:00  iter: 5079  total_loss: 0.4159  loss_cls: 0.1061  loss_box_reg: 0.2982  loss_rpn_cls: 0.008816  loss_rpn_loc: 0.007895    time: 0.4186  last_time: 0.4085  data_time: 0.0343  last_data_time: 0.0521   lr: 0.02  max_mem: 11505M
[32m[11/10 16:32:56 d2.utils.events]: [0m eta: 0:23:52  iter: 5099  total_loss: 0.3629  loss_cls: 0.09126  loss_box_reg: 0.2591  loss_rpn_cls: 0.007629  loss_rpn_loc: 0.009996    time: 0.4186  last_time: 0.4373  data_time: 0.0339  last_data_time: 0.0253   lr: 0.02  max_mem: 11505M
[32m[11/10 16:33:05 d2.utils.events]: [0m eta: 0:23:44  iter: 5119  total_loss: 0.4227  loss_cls: 0.1199  loss_box_reg: 0.2775  loss_rpn_cls: 0.006296  loss_rpn_loc: 0.007509    time: 0.4187  last_time: 0.3876  data_time: 0.0405  last_data_time: 0.0119   lr: 0.02  max_mem: 11505M
[32m[11/10 16:33:13 d2.utils.events]: [0m eta: 0:23:36  iter: 5139  total_loss: 0.4066  loss_cls: 0.1146  loss_box_reg: 0.2692  loss_rpn_cls: 0.008453  loss_rpn_loc: 0.007509    time: 0.4187  last_time: 0.4034  data_time: 0.0327  last_data_time: 0.0245   lr: 0.02  max_mem: 11505M
[32m[11/10 16:33:22 d2.utils.events]: [0m eta: 0:23:28  iter: 5159  total_loss: 0.3961  loss_cls: 0.1043  loss_box_reg: 0.2867  loss_rpn_cls: 0.005843  loss_rpn_loc: 0.007804    time: 0.4187  last_time: 0.4146  data_time: 0.0347  last_data_time: 0.0350   lr: 0.02  max_mem: 11505M
[32m[11/10 16:33:30 d2.utils.events]: [0m eta: 0:23:18  iter: 5179  total_loss: 0.4643  loss_cls: 0.1282  loss_box_reg: 0.3127  loss_rpn_cls: 0.008498  loss_rpn_loc: 0.008563    time: 0.4187  last_time: 0.3989  data_time: 0.0354  last_data_time: 0.0112   lr: 0.02  max_mem: 11505M
[32m[11/10 16:33:39 d2.utils.events]: [0m eta: 0:23:09  iter: 5199  total_loss: 0.4465  loss_cls: 0.1176  loss_box_reg: 0.305  loss_rpn_cls: 0.009233  loss_rpn_loc: 0.007863    time: 0.4188  last_time: 0.4643  data_time: 0.0323  last_data_time: 0.0352   lr: 0.02  max_mem: 11505M
[32m[11/10 16:33:47 d2.utils.events]: [0m eta: 0:23:01  iter: 5219  total_loss: 0.4085  loss_cls: 0.108  loss_box_reg: 0.2924  loss_rpn_cls: 0.006988  loss_rpn_loc: 0.006918    time: 0.4188  last_time: 0.3606  data_time: 0.0302  last_data_time: 0.0085   lr: 0.02  max_mem: 11505M
[32m[11/10 16:33:55 d2.utils.events]: [0m eta: 0:22:52  iter: 5239  total_loss: 0.458  loss_cls: 0.1259  loss_box_reg: 0.2975  loss_rpn_cls: 0.007328  loss_rpn_loc: 0.007544    time: 0.4187  last_time: 0.4180  data_time: 0.0344  last_data_time: 0.0106   lr: 0.02  max_mem: 11505M
[32m[11/10 16:34:04 d2.utils.events]: [0m eta: 0:22:44  iter: 5259  total_loss: 0.4703  loss_cls: 0.1402  loss_box_reg: 0.2923  loss_rpn_cls: 0.008173  loss_rpn_loc: 0.008102    time: 0.4187  last_time: 0.4867  data_time: 0.0366  last_data_time: 0.0687   lr: 0.02  max_mem: 11505M
[32m[11/10 16:34:12 d2.utils.events]: [0m eta: 0:22:35  iter: 5279  total_loss: 0.4427  loss_cls: 0.1259  loss_box_reg: 0.29  loss_rpn_cls: 0.006996  loss_rpn_loc: 0.006254    time: 0.4187  last_time: 0.3836  data_time: 0.0276  last_data_time: 0.0341   lr: 0.02  max_mem: 11505M
[32m[11/10 16:34:21 d2.utils.events]: [0m eta: 0:22:27  iter: 5299  total_loss: 0.4098  loss_cls: 0.1178  loss_box_reg: 0.2664  loss_rpn_cls: 0.007866  loss_rpn_loc: 0.008714    time: 0.4188  last_time: 0.4026  data_time: 0.0301  last_data_time: 0.0118   lr: 0.02  max_mem: 11505M
[32m[11/10 16:34:29 d2.utils.events]: [0m eta: 0:22:20  iter: 5319  total_loss: 0.39  loss_cls: 0.1047  loss_box_reg: 0.2642  loss_rpn_cls: 0.006748  loss_rpn_loc: 0.008506    time: 0.4188  last_time: 0.4235  data_time: 0.0312  last_data_time: 0.0245   lr: 0.02  max_mem: 11505M
[32m[11/10 16:34:38 d2.utils.events]: [0m eta: 0:22:11  iter: 5339  total_loss: 0.3841  loss_cls: 0.1058  loss_box_reg: 0.2738  loss_rpn_cls: 0.005243  loss_rpn_loc: 0.007668    time: 0.4188  last_time: 0.4131  data_time: 0.0319  last_data_time: 0.0298   lr: 0.02  max_mem: 11505M
[32m[11/10 16:34:46 d2.utils.events]: [0m eta: 0:22:04  iter: 5359  total_loss: 0.4558  loss_cls: 0.1098  loss_box_reg: 0.3041  loss_rpn_cls: 0.005829  loss_rpn_loc: 0.008108    time: 0.4189  last_time: 0.3784  data_time: 0.0324  last_data_time: 0.0186   lr: 0.02  max_mem: 11505M
[32m[11/10 16:34:55 d2.utils.events]: [0m eta: 0:21:55  iter: 5379  total_loss: 0.4462  loss_cls: 0.1231  loss_box_reg: 0.3051  loss_rpn_cls: 0.007603  loss_rpn_loc: 0.008418    time: 0.4189  last_time: 0.4529  data_time: 0.0341  last_data_time: 0.0579   lr: 0.02  max_mem: 11505M
[32m[11/10 16:35:04 d2.utils.events]: [0m eta: 0:21:48  iter: 5399  total_loss: 0.4011  loss_cls: 0.1082  loss_box_reg: 0.2862  loss_rpn_cls: 0.005205  loss_rpn_loc: 0.007634    time: 0.4189  last_time: 0.4722  data_time: 0.0323  last_data_time: 0.0382   lr: 0.02  max_mem: 11505M
[32m[11/10 16:35:12 d2.utils.events]: [0m eta: 0:21:39  iter: 5419  total_loss: 0.3703  loss_cls: 0.09003  loss_box_reg: 0.27  loss_rpn_cls: 0.005721  loss_rpn_loc: 0.006188    time: 0.4189  last_time: 0.3733  data_time: 0.0321  last_data_time: 0.0473   lr: 0.02  max_mem: 11505M
[32m[11/10 16:35:20 d2.utils.events]: [0m eta: 0:21:31  iter: 5439  total_loss: 0.3835  loss_cls: 0.09835  loss_box_reg: 0.2683  loss_rpn_cls: 0.005062  loss_rpn_loc: 0.008082    time: 0.4190  last_time: 0.4588  data_time: 0.0347  last_data_time: 0.0318   lr: 0.02  max_mem: 11505M
[32m[11/10 16:35:29 d2.utils.events]: [0m eta: 0:21:22  iter: 5459  total_loss: 0.4166  loss_cls: 0.1215  loss_box_reg: 0.3033  loss_rpn_cls: 0.005981  loss_rpn_loc: 0.005914    time: 0.4190  last_time: 0.4547  data_time: 0.0291  last_data_time: 0.0185   lr: 0.02  max_mem: 11505M
[32m[11/10 16:35:37 d2.utils.events]: [0m eta: 0:21:14  iter: 5479  total_loss: 0.4395  loss_cls: 0.1219  loss_box_reg: 0.2961  loss_rpn_cls: 0.005361  loss_rpn_loc: 0.007313    time: 0.4190  last_time: 0.3987  data_time: 0.0313  last_data_time: 0.0038   lr: 0.02  max_mem: 11505M
[32m[11/10 16:35:46 d2.utils.events]: [0m eta: 0:21:05  iter: 5499  total_loss: 0.4117  loss_cls: 0.1129  loss_box_reg: 0.2923  loss_rpn_cls: 0.005726  loss_rpn_loc: 0.00828    time: 0.4190  last_time: 0.4387  data_time: 0.0326  last_data_time: 0.0390   lr: 0.02  max_mem: 11505M
[32m[11/10 16:35:54 d2.utils.events]: [0m eta: 0:20:58  iter: 5519  total_loss: 0.4163  loss_cls: 0.1082  loss_box_reg: 0.2818  loss_rpn_cls: 0.00552  loss_rpn_loc: 0.008775    time: 0.4190  last_time: 0.4460  data_time: 0.0296  last_data_time: 0.0098   lr: 0.02  max_mem: 11505M
[32m[11/10 16:36:03 d2.utils.events]: [0m eta: 0:20:50  iter: 5539  total_loss: 0.3744  loss_cls: 0.1047  loss_box_reg: 0.2593  loss_rpn_cls: 0.003646  loss_rpn_loc: 0.00592    time: 0.4190  last_time: 0.4021  data_time: 0.0319  last_data_time: 0.0319   lr: 0.02  max_mem: 11505M
[32m[11/10 16:36:11 d2.utils.events]: [0m eta: 0:20:42  iter: 5559  total_loss: 0.4142  loss_cls: 0.111  loss_box_reg: 0.2854  loss_rpn_cls: 0.00449  loss_rpn_loc: 0.007689    time: 0.4190  last_time: 0.4212  data_time: 0.0350  last_data_time: 0.0252   lr: 0.02  max_mem: 11505M
[32m[11/10 16:36:20 d2.utils.events]: [0m eta: 0:20:34  iter: 5579  total_loss: 0.3891  loss_cls: 0.1031  loss_box_reg: 0.2972  loss_rpn_cls: 0.00734  loss_rpn_loc: 0.008661    time: 0.4191  last_time: 0.4295  data_time: 0.0352  last_data_time: 0.0408   lr: 0.02  max_mem: 11505M
[32m[11/10 16:36:29 d2.utils.events]: [0m eta: 0:20:26  iter: 5599  total_loss: 0.4297  loss_cls: 0.1106  loss_box_reg: 0.293  loss_rpn_cls: 0.006471  loss_rpn_loc: 0.009227    time: 0.4191  last_time: 0.4068  data_time: 0.0358  last_data_time: 0.0167   lr: 0.02  max_mem: 11505M
[32m[11/10 16:36:37 d2.utils.events]: [0m eta: 0:20:18  iter: 5619  total_loss: 0.385  loss_cls: 0.1166  loss_box_reg: 0.267  loss_rpn_cls: 0.00596  loss_rpn_loc: 0.007194    time: 0.4192  last_time: 0.4392  data_time: 0.0368  last_data_time: 0.0618   lr: 0.02  max_mem: 11505M
[32m[11/10 16:36:46 d2.utils.events]: [0m eta: 0:20:11  iter: 5639  total_loss: 0.4154  loss_cls: 0.1105  loss_box_reg: 0.2819  loss_rpn_cls: 0.01144  loss_rpn_loc: 0.01062    time: 0.4192  last_time: 0.4409  data_time: 0.0345  last_data_time: 0.0516   lr: 0.02  max_mem: 11505M
[32m[11/10 16:36:54 d2.utils.events]: [0m eta: 0:20:03  iter: 5659  total_loss: 0.3938  loss_cls: 0.107  loss_box_reg: 0.273  loss_rpn_cls: 0.00632  loss_rpn_loc: 0.00821    time: 0.4192  last_time: 0.4282  data_time: 0.0333  last_data_time: 0.0229   lr: 0.02  max_mem: 11505M
[32m[11/10 16:37:03 d2.utils.events]: [0m eta: 0:19:54  iter: 5679  total_loss: 0.3772  loss_cls: 0.1057  loss_box_reg: 0.2581  loss_rpn_cls: 0.00661  loss_rpn_loc: 0.006842    time: 0.4193  last_time: 0.3828  data_time: 0.0334  last_data_time: 0.0077   lr: 0.02  max_mem: 11505M
[32m[11/10 16:37:12 d2.utils.events]: [0m eta: 0:19:46  iter: 5699  total_loss: 0.4021  loss_cls: 0.1053  loss_box_reg: 0.2794  loss_rpn_cls: 0.005514  loss_rpn_loc: 0.006315    time: 0.4193  last_time: 0.4324  data_time: 0.0353  last_data_time: 0.0404   lr: 0.02  max_mem: 11505M
[32m[11/10 16:37:20 d2.utils.events]: [0m eta: 0:19:38  iter: 5719  total_loss: 0.3872  loss_cls: 0.108  loss_box_reg: 0.2643  loss_rpn_cls: 0.005694  loss_rpn_loc: 0.006409    time: 0.4194  last_time: 0.4056  data_time: 0.0333  last_data_time: 0.0069   lr: 0.02  max_mem: 11505M
[32m[11/10 16:37:29 d2.utils.events]: [0m eta: 0:19:30  iter: 5739  total_loss: 0.3807  loss_cls: 0.1022  loss_box_reg: 0.2722  loss_rpn_cls: 0.005395  loss_rpn_loc: 0.007821    time: 0.4194  last_time: 0.3750  data_time: 0.0348  last_data_time: 0.0372   lr: 0.02  max_mem: 11505M
[32m[11/10 16:37:37 d2.utils.events]: [0m eta: 0:19:23  iter: 5759  total_loss: 0.35  loss_cls: 0.1093  loss_box_reg: 0.2453  loss_rpn_cls: 0.006263  loss_rpn_loc: 0.007183    time: 0.4194  last_time: 0.4314  data_time: 0.0310  last_data_time: 0.0183   lr: 0.02  max_mem: 11505M
[32m[11/10 16:37:46 d2.utils.events]: [0m eta: 0:19:13  iter: 5779  total_loss: 0.389  loss_cls: 0.09394  loss_box_reg: 0.2572  loss_rpn_cls: 0.005207  loss_rpn_loc: 0.007578    time: 0.4194  last_time: 0.4492  data_time: 0.0339  last_data_time: 0.0583   lr: 0.02  max_mem: 11505M
[32m[11/10 16:37:54 d2.utils.events]: [0m eta: 0:19:05  iter: 5799  total_loss: 0.3443  loss_cls: 0.08192  loss_box_reg: 0.2473  loss_rpn_cls: 0.005798  loss_rpn_loc: 0.006331    time: 0.4194  last_time: 0.4449  data_time: 0.0320  last_data_time: 0.0419   lr: 0.02  max_mem: 11505M
[32m[11/10 16:38:02 d2.utils.events]: [0m eta: 0:18:55  iter: 5819  total_loss: 0.3963  loss_cls: 0.09546  loss_box_reg: 0.2844  loss_rpn_cls: 0.006842  loss_rpn_loc: 0.008521    time: 0.4194  last_time: 0.4349  data_time: 0.0276  last_data_time: 0.0074   lr: 0.02  max_mem: 11505M
[32m[11/10 16:38:11 d2.utils.events]: [0m eta: 0:18:48  iter: 5839  total_loss: 0.3609  loss_cls: 0.08866  loss_box_reg: 0.2639  loss_rpn_cls: 0.006553  loss_rpn_loc: 0.008528    time: 0.4194  last_time: 0.3960  data_time: 0.0341  last_data_time: 0.0387   lr: 0.02  max_mem: 11505M
[32m[11/10 16:38:19 d2.utils.events]: [0m eta: 0:18:39  iter: 5859  total_loss: 0.3401  loss_cls: 0.08916  loss_box_reg: 0.2319  loss_rpn_cls: 0.006294  loss_rpn_loc: 0.007249    time: 0.4194  last_time: 0.4709  data_time: 0.0284  last_data_time: 0.0502   lr: 0.02  max_mem: 11505M
[32m[11/10 16:38:28 d2.utils.events]: [0m eta: 0:18:30  iter: 5879  total_loss: 0.394  loss_cls: 0.1029  loss_box_reg: 0.273  loss_rpn_cls: 0.005243  loss_rpn_loc: 0.007559    time: 0.4194  last_time: 0.4086  data_time: 0.0332  last_data_time: 0.0230   lr: 0.02  max_mem: 11505M
[32m[11/10 16:38:36 d2.utils.events]: [0m eta: 0:18:21  iter: 5899  total_loss: 0.3833  loss_cls: 0.1095  loss_box_reg: 0.2741  loss_rpn_cls: 0.004282  loss_rpn_loc: 0.006389    time: 0.4194  last_time: 0.4384  data_time: 0.0291  last_data_time: 0.0237   lr: 0.02  max_mem: 11505M
[32m[11/10 16:38:45 d2.utils.events]: [0m eta: 0:18:14  iter: 5919  total_loss: 0.4145  loss_cls: 0.1226  loss_box_reg: 0.2633  loss_rpn_cls: 0.006282  loss_rpn_loc: 0.006651    time: 0.4195  last_time: 0.4418  data_time: 0.0345  last_data_time: 0.0207   lr: 0.02  max_mem: 11505M
[32m[11/10 16:38:53 d2.utils.events]: [0m eta: 0:18:05  iter: 5939  total_loss: 0.453  loss_cls: 0.1326  loss_box_reg: 0.3026  loss_rpn_cls: 0.007929  loss_rpn_loc: 0.009784    time: 0.4195  last_time: 0.5134  data_time: 0.0360  last_data_time: 0.0735   lr: 0.02  max_mem: 11505M
[32m[11/10 16:39:02 d2.utils.events]: [0m eta: 0:17:56  iter: 5959  total_loss: 0.3997  loss_cls: 0.1026  loss_box_reg: 0.2764  loss_rpn_cls: 0.007231  loss_rpn_loc: 0.008416    time: 0.4195  last_time: 0.4652  data_time: 0.0323  last_data_time: 0.0799   lr: 0.02  max_mem: 11505M
[32m[11/10 16:39:10 d2.utils.events]: [0m eta: 0:17:48  iter: 5979  total_loss: 0.4017  loss_cls: 0.1186  loss_box_reg: 0.2837  loss_rpn_cls: 0.00664  loss_rpn_loc: 0.0078    time: 0.4195  last_time: 0.4117  data_time: 0.0316  last_data_time: 0.0155   lr: 0.02  max_mem: 11505M
[32m[11/10 16:39:19 d2.utils.events]: [0m eta: 0:17:39  iter: 5999  total_loss: 0.3651  loss_cls: 0.1035  loss_box_reg: 0.2546  loss_rpn_cls: 0.004833  loss_rpn_loc: 0.006763    time: 0.4195  last_time: 0.3978  data_time: 0.0367  last_data_time: 0.0125   lr: 0.02  max_mem: 11505M
[32m[11/10 16:39:27 d2.utils.events]: [0m eta: 0:17:32  iter: 6019  total_loss: 0.3684  loss_cls: 0.1024  loss_box_reg: 0.2514  loss_rpn_cls: 0.006334  loss_rpn_loc: 0.006562    time: 0.4196  last_time: 0.4057  data_time: 0.0414  last_data_time: 0.0372   lr: 0.02  max_mem: 11505M
[32m[11/10 16:39:36 d2.utils.events]: [0m eta: 0:17:25  iter: 6039  total_loss: 0.3783  loss_cls: 0.08526  loss_box_reg: 0.2547  loss_rpn_cls: 0.006848  loss_rpn_loc: 0.008723    time: 0.4196  last_time: 0.4433  data_time: 0.0312  last_data_time: 0.0485   lr: 0.02  max_mem: 11505M
[32m[11/10 16:39:45 d2.utils.events]: [0m eta: 0:17:15  iter: 6059  total_loss: 0.3929  loss_cls: 0.1054  loss_box_reg: 0.2748  loss_rpn_cls: 0.006358  loss_rpn_loc: 0.009146    time: 0.4196  last_time: 0.4679  data_time: 0.0346  last_data_time: 0.0662   lr: 0.02  max_mem: 11505M
[32m[11/10 16:39:53 d2.utils.events]: [0m eta: 0:17:08  iter: 6079  total_loss: 0.3836  loss_cls: 0.1031  loss_box_reg: 0.2638  loss_rpn_cls: 0.005453  loss_rpn_loc: 0.007386    time: 0.4197  last_time: 0.4447  data_time: 0.0302  last_data_time: 0.0664   lr: 0.02  max_mem: 11505M
[32m[11/10 16:40:02 d2.utils.events]: [0m eta: 0:17:00  iter: 6099  total_loss: 0.391  loss_cls: 0.09773  loss_box_reg: 0.2749  loss_rpn_cls: 0.004091  loss_rpn_loc: 0.006809    time: 0.4197  last_time: 0.3805  data_time: 0.0328  last_data_time: 0.0119   lr: 0.02  max_mem: 11505M
[32m[11/10 16:40:10 d2.utils.events]: [0m eta: 0:16:51  iter: 6119  total_loss: 0.3542  loss_cls: 0.08992  loss_box_reg: 0.2442  loss_rpn_cls: 0.004956  loss_rpn_loc: 0.006919    time: 0.4197  last_time: 0.3987  data_time: 0.0338  last_data_time: 0.0170   lr: 0.02  max_mem: 11505M
[32m[11/10 16:40:19 d2.utils.events]: [0m eta: 0:16:44  iter: 6139  total_loss: 0.3487  loss_cls: 0.07715  loss_box_reg: 0.2595  loss_rpn_cls: 0.005463  loss_rpn_loc: 0.006318    time: 0.4198  last_time: 0.4315  data_time: 0.0415  last_data_time: 0.0576   lr: 0.02  max_mem: 11505M
[32m[11/10 16:40:27 d2.utils.events]: [0m eta: 0:16:36  iter: 6159  total_loss: 0.4235  loss_cls: 0.1209  loss_box_reg: 0.2801  loss_rpn_cls: 0.007444  loss_rpn_loc: 0.007684    time: 0.4198  last_time: 0.4815  data_time: 0.0333  last_data_time: 0.0447   lr: 0.02  max_mem: 11505M
[32m[11/10 16:40:36 d2.utils.events]: [0m eta: 0:16:28  iter: 6179  total_loss: 0.4127  loss_cls: 0.1222  loss_box_reg: 0.2723  loss_rpn_cls: 0.005142  loss_rpn_loc: 0.005907    time: 0.4198  last_time: 0.4405  data_time: 0.0324  last_data_time: 0.0394   lr: 0.02  max_mem: 11505M
[32m[11/10 16:40:45 d2.utils.events]: [0m eta: 0:16:20  iter: 6199  total_loss: 0.4008  loss_cls: 0.1073  loss_box_reg: 0.2824  loss_rpn_cls: 0.00503  loss_rpn_loc: 0.006709    time: 0.4199  last_time: 0.3972  data_time: 0.0318  last_data_time: 0.0028   lr: 0.02  max_mem: 11505M
[32m[11/10 16:40:53 d2.utils.events]: [0m eta: 0:16:11  iter: 6219  total_loss: 0.3581  loss_cls: 0.09312  loss_box_reg: 0.2517  loss_rpn_cls: 0.006327  loss_rpn_loc: 0.005193    time: 0.4199  last_time: 0.4374  data_time: 0.0346  last_data_time: 0.0098   lr: 0.02  max_mem: 11505M
[32m[11/10 16:41:02 d2.utils.events]: [0m eta: 0:16:04  iter: 6239  total_loss: 0.3882  loss_cls: 0.104  loss_box_reg: 0.2773  loss_rpn_cls: 0.00635  loss_rpn_loc: 0.00826    time: 0.4199  last_time: 0.4286  data_time: 0.0335  last_data_time: 0.0535   lr: 0.02  max_mem: 11505M
[32m[11/10 16:41:11 d2.utils.events]: [0m eta: 0:15:56  iter: 6259  total_loss: 0.3843  loss_cls: 0.1081  loss_box_reg: 0.2607  loss_rpn_cls: 0.005571  loss_rpn_loc: 0.006872    time: 0.4200  last_time: 0.3588  data_time: 0.0363  last_data_time: 0.0113   lr: 0.02  max_mem: 11505M
[32m[11/10 16:41:19 d2.utils.events]: [0m eta: 0:15:47  iter: 6279  total_loss: 0.3772  loss_cls: 0.09452  loss_box_reg: 0.2646  loss_rpn_cls: 0.006065  loss_rpn_loc: 0.006898    time: 0.4200  last_time: 0.4088  data_time: 0.0343  last_data_time: 0.0192   lr: 0.02  max_mem: 11505M
[32m[11/10 16:41:28 d2.utils.events]: [0m eta: 0:15:39  iter: 6299  total_loss: 0.3359  loss_cls: 0.09325  loss_box_reg: 0.2355  loss_rpn_cls: 0.005109  loss_rpn_loc: 0.006956    time: 0.4200  last_time: 0.4210  data_time: 0.0334  last_data_time: 0.0349   lr: 0.02  max_mem: 11505M
[32m[11/10 16:41:36 d2.utils.events]: [0m eta: 0:15:30  iter: 6319  total_loss: 0.3824  loss_cls: 0.1034  loss_box_reg: 0.2668  loss_rpn_cls: 0.006122  loss_rpn_loc: 0.007143    time: 0.4200  last_time: 0.4855  data_time: 0.0404  last_data_time: 0.0655   lr: 0.02  max_mem: 11505M
[32m[11/10 16:41:45 d2.utils.events]: [0m eta: 0:15:22  iter: 6339  total_loss: 0.3771  loss_cls: 0.09424  loss_box_reg: 0.2745  loss_rpn_cls: 0.004382  loss_rpn_loc: 0.008196    time: 0.4200  last_time: 0.4695  data_time: 0.0283  last_data_time: 0.0604   lr: 0.02  max_mem: 11505M
[32m[11/10 16:41:53 d2.utils.events]: [0m eta: 0:15:12  iter: 6359  total_loss: 0.3921  loss_cls: 0.1097  loss_box_reg: 0.279  loss_rpn_cls: 0.004823  loss_rpn_loc: 0.006397    time: 0.4200  last_time: 0.4096  data_time: 0.0319  last_data_time: 0.0331   lr: 0.02  max_mem: 11505M
[32m[11/10 16:42:02 d2.utils.events]: [0m eta: 0:15:04  iter: 6379  total_loss: 0.406  loss_cls: 0.1024  loss_box_reg: 0.2901  loss_rpn_cls: 0.00604  loss_rpn_loc: 0.005815    time: 0.4200  last_time: 0.4553  data_time: 0.0342  last_data_time: 0.0810   lr: 0.02  max_mem: 11505M
[32m[11/10 16:42:10 d2.utils.events]: [0m eta: 0:14:55  iter: 6399  total_loss: 0.4417  loss_cls: 0.1159  loss_box_reg: 0.2856  loss_rpn_cls: 0.006193  loss_rpn_loc: 0.007743    time: 0.4201  last_time: 0.4033  data_time: 0.0306  last_data_time: 0.0332   lr: 0.02  max_mem: 11505M
[32m[11/10 16:42:19 d2.utils.events]: [0m eta: 0:14:47  iter: 6419  total_loss: 0.3531  loss_cls: 0.09582  loss_box_reg: 0.2624  loss_rpn_cls: 0.004984  loss_rpn_loc: 0.007449    time: 0.4201  last_time: 0.4014  data_time: 0.0361  last_data_time: 0.0608   lr: 0.02  max_mem: 11505M
[32m[11/10 16:42:27 d2.utils.events]: [0m eta: 0:14:39  iter: 6439  total_loss: 0.4087  loss_cls: 0.1244  loss_box_reg: 0.2782  loss_rpn_cls: 0.007896  loss_rpn_loc: 0.008767    time: 0.4202  last_time: 0.4448  data_time: 0.0382  last_data_time: 0.0401   lr: 0.02  max_mem: 11505M
[32m[11/10 16:42:36 d2.utils.events]: [0m eta: 0:14:30  iter: 6459  total_loss: 0.3777  loss_cls: 0.09918  loss_box_reg: 0.2569  loss_rpn_cls: 0.006264  loss_rpn_loc: 0.0068    time: 0.4202  last_time: 0.4061  data_time: 0.0339  last_data_time: 0.0223   lr: 0.02  max_mem: 11505M
[32m[11/10 16:42:44 d2.utils.events]: [0m eta: 0:14:22  iter: 6479  total_loss: 0.4096  loss_cls: 0.1112  loss_box_reg: 0.2864  loss_rpn_cls: 0.005722  loss_rpn_loc: 0.007002    time: 0.4202  last_time: 0.3903  data_time: 0.0324  last_data_time: 0.0079   lr: 0.02  max_mem: 11505M
[32m[11/10 16:42:53 d2.utils.events]: [0m eta: 0:14:13  iter: 6499  total_loss: 0.4117  loss_cls: 0.1035  loss_box_reg: 0.2841  loss_rpn_cls: 0.004566  loss_rpn_loc: 0.007508    time: 0.4202  last_time: 0.4774  data_time: 0.0364  last_data_time: 0.0652   lr: 0.02  max_mem: 11505M
[32m[11/10 16:43:01 d2.utils.events]: [0m eta: 0:14:05  iter: 6519  total_loss: 0.3423  loss_cls: 0.08692  loss_box_reg: 0.2418  loss_rpn_cls: 0.004464  loss_rpn_loc: 0.006683    time: 0.4202  last_time: 0.4121  data_time: 0.0326  last_data_time: 0.0388   lr: 0.02  max_mem: 11505M
[32m[11/10 16:43:10 d2.utils.events]: [0m eta: 0:13:56  iter: 6539  total_loss: 0.4375  loss_cls: 0.1219  loss_box_reg: 0.2877  loss_rpn_cls: 0.008685  loss_rpn_loc: 0.008867    time: 0.4202  last_time: 0.5144  data_time: 0.0356  last_data_time: 0.0564   lr: 0.02  max_mem: 11505M
[32m[11/10 16:43:18 d2.utils.events]: [0m eta: 0:13:47  iter: 6559  total_loss: 0.3899  loss_cls: 0.09887  loss_box_reg: 0.2737  loss_rpn_cls: 0.005067  loss_rpn_loc: 0.007485    time: 0.4202  last_time: 0.3929  data_time: 0.0308  last_data_time: 0.0265   lr: 0.02  max_mem: 11505M
[32m[11/10 16:43:27 d2.utils.events]: [0m eta: 0:13:38  iter: 6579  total_loss: 0.4158  loss_cls: 0.1138  loss_box_reg: 0.2847  loss_rpn_cls: 0.004618  loss_rpn_loc: 0.007499    time: 0.4202  last_time: 0.4691  data_time: 0.0362  last_data_time: 0.0675   lr: 0.02  max_mem: 11505M
[32m[11/10 16:43:35 d2.utils.events]: [0m eta: 0:13:29  iter: 6599  total_loss: 0.3919  loss_cls: 0.0988  loss_box_reg: 0.276  loss_rpn_cls: 0.004501  loss_rpn_loc: 0.006345    time: 0.4203  last_time: 0.3970  data_time: 0.0346  last_data_time: 0.0072   lr: 0.02  max_mem: 11505M
[32m[11/10 16:43:44 d2.utils.events]: [0m eta: 0:13:20  iter: 6619  total_loss: 0.392  loss_cls: 0.09738  loss_box_reg: 0.2628  loss_rpn_cls: 0.006003  loss_rpn_loc: 0.006313    time: 0.4202  last_time: 0.4142  data_time: 0.0321  last_data_time: 0.0481   lr: 0.02  max_mem: 11505M
[32m[11/10 16:43:53 d2.utils.events]: [0m eta: 0:13:12  iter: 6639  total_loss: 0.3672  loss_cls: 0.09095  loss_box_reg: 0.259  loss_rpn_cls: 0.00464  loss_rpn_loc: 0.005354    time: 0.4203  last_time: 0.4835  data_time: 0.0351  last_data_time: 0.0472   lr: 0.02  max_mem: 11505M
[32m[11/10 16:44:01 d2.utils.events]: [0m eta: 0:13:04  iter: 6659  total_loss: 0.3839  loss_cls: 0.1068  loss_box_reg: 0.2706  loss_rpn_cls: 0.004965  loss_rpn_loc: 0.006115    time: 0.4203  last_time: 0.4038  data_time: 0.0323  last_data_time: 0.0091   lr: 0.02  max_mem: 11505M
[32m[11/10 16:44:10 d2.utils.events]: [0m eta: 0:12:55  iter: 6679  total_loss: 0.3642  loss_cls: 0.09411  loss_box_reg: 0.2619  loss_rpn_cls: 0.004417  loss_rpn_loc: 0.006804    time: 0.4204  last_time: 0.4177  data_time: 0.0371  last_data_time: 0.0506   lr: 0.02  max_mem: 11505M
[32m[11/10 16:44:19 d2.utils.events]: [0m eta: 0:12:47  iter: 6699  total_loss: 0.3652  loss_cls: 0.09361  loss_box_reg: 0.251  loss_rpn_cls: 0.004823  loss_rpn_loc: 0.00891    time: 0.4204  last_time: 0.4690  data_time: 0.0394  last_data_time: 0.0705   lr: 0.02  max_mem: 11505M
[32m[11/10 16:44:27 d2.utils.events]: [0m eta: 0:12:38  iter: 6719  total_loss: 0.3999  loss_cls: 0.1118  loss_box_reg: 0.2763  loss_rpn_cls: 0.005112  loss_rpn_loc: 0.007364    time: 0.4205  last_time: 0.4143  data_time: 0.0290  last_data_time: 0.0286   lr: 0.02  max_mem: 11505M
[32m[11/10 16:44:36 d2.utils.events]: [0m eta: 0:12:29  iter: 6739  total_loss: 0.4087  loss_cls: 0.1135  loss_box_reg: 0.2911  loss_rpn_cls: 0.006926  loss_rpn_loc: 0.009094    time: 0.4205  last_time: 0.3942  data_time: 0.0368  last_data_time: 0.0039   lr: 0.02  max_mem: 11505M
[32m[11/10 16:44:44 d2.utils.events]: [0m eta: 0:12:22  iter: 6759  total_loss: 0.3681  loss_cls: 0.115  loss_box_reg: 0.2324  loss_rpn_cls: 0.008218  loss_rpn_loc: 0.009435    time: 0.4205  last_time: 0.4132  data_time: 0.0342  last_data_time: 0.0320   lr: 0.02  max_mem: 11505M
[32m[11/10 16:44:53 d2.utils.events]: [0m eta: 0:12:13  iter: 6779  total_loss: 0.3524  loss_cls: 0.09434  loss_box_reg: 0.2451  loss_rpn_cls: 0.006064  loss_rpn_loc: 0.007638    time: 0.4205  last_time: 0.4112  data_time: 0.0382  last_data_time: 0.0273   lr: 0.02  max_mem: 11505M
[32m[11/10 16:45:02 d2.utils.events]: [0m eta: 0:12:05  iter: 6799  total_loss: 0.3641  loss_cls: 0.09804  loss_box_reg: 0.2522  loss_rpn_cls: 0.007131  loss_rpn_loc: 0.006472    time: 0.4206  last_time: 0.4055  data_time: 0.0301  last_data_time: 0.0451   lr: 0.02  max_mem: 11505M
[32m[11/10 16:45:10 d2.utils.events]: [0m eta: 0:11:57  iter: 6819  total_loss: 0.3656  loss_cls: 0.09966  loss_box_reg: 0.2485  loss_rpn_cls: 0.004901  loss_rpn_loc: 0.008394    time: 0.4206  last_time: 0.4558  data_time: 0.0339  last_data_time: 0.0185   lr: 0.02  max_mem: 11505M
[32m[11/10 16:45:19 d2.utils.events]: [0m eta: 0:11:48  iter: 6839  total_loss: 0.3995  loss_cls: 0.1004  loss_box_reg: 0.2757  loss_rpn_cls: 0.007926  loss_rpn_loc: 0.006706    time: 0.4206  last_time: 0.4751  data_time: 0.0313  last_data_time: 0.0402   lr: 0.02  max_mem: 11505M
[32m[11/10 16:45:27 d2.utils.events]: [0m eta: 0:11:40  iter: 6859  total_loss: 0.4088  loss_cls: 0.1193  loss_box_reg: 0.2725  loss_rpn_cls: 0.007133  loss_rpn_loc: 0.009968    time: 0.4206  last_time: 0.3910  data_time: 0.0319  last_data_time: 0.0155   lr: 0.02  max_mem: 11505M
[32m[11/10 16:45:36 d2.utils.events]: [0m eta: 0:11:32  iter: 6879  total_loss: 0.4496  loss_cls: 0.1239  loss_box_reg: 0.3106  loss_rpn_cls: 0.006202  loss_rpn_loc: 0.008054    time: 0.4207  last_time: 0.3901  data_time: 0.0331  last_data_time: 0.0081   lr: 0.02  max_mem: 11505M
[32m[11/10 16:45:44 d2.utils.events]: [0m eta: 0:11:24  iter: 6899  total_loss: 0.3771  loss_cls: 0.101  loss_box_reg: 0.2834  loss_rpn_cls: 0.004411  loss_rpn_loc: 0.006277    time: 0.4207  last_time: 0.5009  data_time: 0.0343  last_data_time: 0.0636   lr: 0.02  max_mem: 11505M
[32m[11/10 16:45:53 d2.utils.events]: [0m eta: 0:11:15  iter: 6919  total_loss: 0.3911  loss_cls: 0.0968  loss_box_reg: 0.2639  loss_rpn_cls: 0.005233  loss_rpn_loc: 0.006693    time: 0.4207  last_time: 0.4674  data_time: 0.0341  last_data_time: 0.0259   lr: 0.02  max_mem: 11505M
[32m[11/10 16:46:02 d2.utils.events]: [0m eta: 0:11:07  iter: 6939  total_loss: 0.4226  loss_cls: 0.1105  loss_box_reg: 0.295  loss_rpn_cls: 0.005008  loss_rpn_loc: 0.007675    time: 0.4207  last_time: 0.4079  data_time: 0.0335  last_data_time: 0.0138   lr: 0.02  max_mem: 11505M
[32m[11/10 16:46:10 d2.utils.events]: [0m eta: 0:10:59  iter: 6959  total_loss: 0.4017  loss_cls: 0.1048  loss_box_reg: 0.2715  loss_rpn_cls: 0.003963  loss_rpn_loc: 0.005766    time: 0.4207  last_time: 0.4967  data_time: 0.0348  last_data_time: 0.0821   lr: 0.02  max_mem: 11505M
[32m[11/10 16:46:19 d2.utils.events]: [0m eta: 0:10:51  iter: 6979  total_loss: 0.4016  loss_cls: 0.1115  loss_box_reg: 0.2809  loss_rpn_cls: 0.005967  loss_rpn_loc: 0.00745    time: 0.4208  last_time: 0.4820  data_time: 0.0382  last_data_time: 0.0472   lr: 0.02  max_mem: 11505M
[32m[11/10 16:46:28 d2.utils.events]: [0m eta: 0:10:43  iter: 6999  total_loss: 0.3662  loss_cls: 0.09759  loss_box_reg: 0.2648  loss_rpn_cls: 0.004623  loss_rpn_loc: 0.007383    time: 0.4208  last_time: 0.3880  data_time: 0.0322  last_data_time: 0.0148   lr: 0.02  max_mem: 11505M
[32m[11/10 16:46:36 d2.utils.events]: [0m eta: 0:10:35  iter: 7019  total_loss: 0.3592  loss_cls: 0.0941  loss_box_reg: 0.2461  loss_rpn_cls: 0.003759  loss_rpn_loc: 0.008028    time: 0.4209  last_time: 0.4254  data_time: 0.0345  last_data_time: 0.0224   lr: 0.02  max_mem: 11505M
[32m[11/10 16:46:45 d2.utils.events]: [0m eta: 0:10:26  iter: 7039  total_loss: 0.3873  loss_cls: 0.1037  loss_box_reg: 0.2569  loss_rpn_cls: 0.0037  loss_rpn_loc: 0.005928    time: 0.4209  last_time: 0.5001  data_time: 0.0359  last_data_time: 0.0762   lr: 0.02  max_mem: 11505M
[32m[11/10 16:46:53 d2.utils.events]: [0m eta: 0:10:17  iter: 7059  total_loss: 0.3656  loss_cls: 0.08259  loss_box_reg: 0.2598  loss_rpn_cls: 0.00505  loss_rpn_loc: 0.006983    time: 0.4209  last_time: 0.4357  data_time: 0.0305  last_data_time: 0.0174   lr: 0.02  max_mem: 11505M
[32m[11/10 16:47:02 d2.utils.events]: [0m eta: 0:10:08  iter: 7079  total_loss: 0.4058  loss_cls: 0.1136  loss_box_reg: 0.2798  loss_rpn_cls: 0.00678  loss_rpn_loc: 0.007225    time: 0.4209  last_time: 0.4383  data_time: 0.0409  last_data_time: 0.0633   lr: 0.02  max_mem: 11505M
[32m[11/10 16:47:11 d2.utils.events]: [0m eta: 0:10:00  iter: 7099  total_loss: 0.3719  loss_cls: 0.09544  loss_box_reg: 0.2643  loss_rpn_cls: 0.004743  loss_rpn_loc: 0.006423    time: 0.4209  last_time: 0.4422  data_time: 0.0296  last_data_time: 0.0355   lr: 0.02  max_mem: 11505M
[32m[11/10 16:47:19 d2.utils.events]: [0m eta: 0:09:52  iter: 7119  total_loss: 0.3611  loss_cls: 0.09757  loss_box_reg: 0.2408  loss_rpn_cls: 0.003831  loss_rpn_loc: 0.007108    time: 0.4210  last_time: 0.4298  data_time: 0.0341  last_data_time: 0.0366   lr: 0.02  max_mem: 11505M
[32m[11/10 16:47:28 d2.utils.events]: [0m eta: 0:09:43  iter: 7139  total_loss: 0.3916  loss_cls: 0.1004  loss_box_reg: 0.2634  loss_rpn_cls: 0.006658  loss_rpn_loc: 0.007282    time: 0.4210  last_time: 0.3731  data_time: 0.0346  last_data_time: 0.0310   lr: 0.02  max_mem: 11505M
[32m[11/10 16:47:36 d2.utils.events]: [0m eta: 0:09:33  iter: 7159  total_loss: 0.3702  loss_cls: 0.08568  loss_box_reg: 0.259  loss_rpn_cls: 0.004293  loss_rpn_loc: 0.009709    time: 0.4210  last_time: 0.4220  data_time: 0.0357  last_data_time: 0.0545   lr: 0.02  max_mem: 11505M
[32m[11/10 16:47:45 d2.utils.events]: [0m eta: 0:09:24  iter: 7179  total_loss: 0.3889  loss_cls: 0.09118  loss_box_reg: 0.2802  loss_rpn_cls: 0.003896  loss_rpn_loc: 0.006081    time: 0.4210  last_time: 0.3887  data_time: 0.0365  last_data_time: 0.0065   lr: 0.02  max_mem: 11505M
[32m[11/10 16:47:53 d2.utils.events]: [0m eta: 0:09:16  iter: 7199  total_loss: 0.39  loss_cls: 0.1051  loss_box_reg: 0.2716  loss_rpn_cls: 0.00515  loss_rpn_loc: 0.007664    time: 0.4210  last_time: 0.4019  data_time: 0.0325  last_data_time: 0.0084   lr: 0.02  max_mem: 11505M
[32m[11/10 16:48:02 d2.utils.events]: [0m eta: 0:09:07  iter: 7219  total_loss: 0.3876  loss_cls: 0.09706  loss_box_reg: 0.266  loss_rpn_cls: 0.006381  loss_rpn_loc: 0.008407    time: 0.4210  last_time: 0.3929  data_time: 0.0378  last_data_time: 0.0305   lr: 0.02  max_mem: 11505M
[32m[11/10 16:48:10 d2.utils.events]: [0m eta: 0:08:58  iter: 7239  total_loss: 0.3636  loss_cls: 0.09143  loss_box_reg: 0.2484  loss_rpn_cls: 0.005518  loss_rpn_loc: 0.006309    time: 0.4210  last_time: 0.4192  data_time: 0.0294  last_data_time: 0.0215   lr: 0.02  max_mem: 11505M
[32m[11/10 16:48:19 d2.utils.events]: [0m eta: 0:08:48  iter: 7259  total_loss: 0.3602  loss_cls: 0.09913  loss_box_reg: 0.2643  loss_rpn_cls: 0.003969  loss_rpn_loc: 0.006428    time: 0.4210  last_time: 0.4122  data_time: 0.0363  last_data_time: 0.0386   lr: 0.02  max_mem: 11505M
[32m[11/10 16:48:27 d2.utils.events]: [0m eta: 0:08:40  iter: 7279  total_loss: 0.3745  loss_cls: 0.09264  loss_box_reg: 0.2598  loss_rpn_cls: 0.003962  loss_rpn_loc: 0.007151    time: 0.4211  last_time: 0.4028  data_time: 0.0366  last_data_time: 0.0332   lr: 0.02  max_mem: 11505M
[32m[11/10 16:48:36 d2.utils.events]: [0m eta: 0:08:31  iter: 7299  total_loss: 0.3471  loss_cls: 0.09185  loss_box_reg: 0.236  loss_rpn_cls: 0.003395  loss_rpn_loc: 0.006227    time: 0.4211  last_time: 0.3865  data_time: 0.0339  last_data_time: 0.0324   lr: 0.02  max_mem: 11505M
[32m[11/10 16:48:44 d2.utils.events]: [0m eta: 0:08:23  iter: 7319  total_loss: 0.4119  loss_cls: 0.1068  loss_box_reg: 0.2872  loss_rpn_cls: 0.006015  loss_rpn_loc: 0.007544    time: 0.4211  last_time: 0.4124  data_time: 0.0352  last_data_time: 0.0091   lr: 0.02  max_mem: 11505M
[32m[11/10 16:48:53 d2.utils.events]: [0m eta: 0:08:14  iter: 7339  total_loss: 0.3702  loss_cls: 0.09813  loss_box_reg: 0.2581  loss_rpn_cls: 0.004902  loss_rpn_loc: 0.007585    time: 0.4211  last_time: 0.3857  data_time: 0.0336  last_data_time: 0.0478   lr: 0.02  max_mem: 11505M
[32m[11/10 16:49:01 d2.utils.events]: [0m eta: 0:08:06  iter: 7359  total_loss: 0.3934  loss_cls: 0.09226  loss_box_reg: 0.2872  loss_rpn_cls: 0.004722  loss_rpn_loc: 0.006471    time: 0.4211  last_time: 0.4304  data_time: 0.0338  last_data_time: 0.0331   lr: 0.02  max_mem: 11505M
[32m[11/10 16:49:10 d2.utils.events]: [0m eta: 0:07:57  iter: 7379  total_loss: 0.3895  loss_cls: 0.1009  loss_box_reg: 0.284  loss_rpn_cls: 0.006742  loss_rpn_loc: 0.008724    time: 0.4211  last_time: 0.4349  data_time: 0.0293  last_data_time: 0.0262   lr: 0.02  max_mem: 11505M
[32m[11/10 16:49:19 d2.utils.events]: [0m eta: 0:07:49  iter: 7399  total_loss: 0.408  loss_cls: 0.1106  loss_box_reg: 0.2764  loss_rpn_cls: 0.005466  loss_rpn_loc: 0.007365    time: 0.4212  last_time: 0.4836  data_time: 0.0335  last_data_time: 0.0584   lr: 0.02  max_mem: 11505M
[32m[11/10 16:49:27 d2.utils.events]: [0m eta: 0:07:40  iter: 7419  total_loss: 0.4222  loss_cls: 0.1105  loss_box_reg: 0.2819  loss_rpn_cls: 0.006295  loss_rpn_loc: 0.006699    time: 0.4212  last_time: 0.3940  data_time: 0.0341  last_data_time: 0.0227   lr: 0.02  max_mem: 11505M
[32m[11/10 16:49:36 d2.utils.events]: [0m eta: 0:07:31  iter: 7439  total_loss: 0.3745  loss_cls: 0.09096  loss_box_reg: 0.2585  loss_rpn_cls: 0.006276  loss_rpn_loc: 0.007349    time: 0.4212  last_time: 0.3865  data_time: 0.0319  last_data_time: 0.0328   lr: 0.02  max_mem: 11505M
[32m[11/10 16:49:44 d2.utils.events]: [0m eta: 0:07:23  iter: 7459  total_loss: 0.3294  loss_cls: 0.09126  loss_box_reg: 0.2278  loss_rpn_cls: 0.005066  loss_rpn_loc: 0.005674    time: 0.4212  last_time: 0.3939  data_time: 0.0318  last_data_time: 0.0227   lr: 0.02  max_mem: 11505M
[32m[11/10 16:49:52 d2.utils.events]: [0m eta: 0:07:14  iter: 7479  total_loss: 0.3895  loss_cls: 0.09785  loss_box_reg: 0.2763  loss_rpn_cls: 0.005362  loss_rpn_loc: 0.006936    time: 0.4212  last_time: 0.4101  data_time: 0.0323  last_data_time: 0.0362   lr: 0.02  max_mem: 11505M
[32m[11/10 16:50:01 d2.utils.events]: [0m eta: 0:07:06  iter: 7499  total_loss: 0.322  loss_cls: 0.07532  loss_box_reg: 0.2362  loss_rpn_cls: 0.00512  loss_rpn_loc: 0.006267    time: 0.4212  last_time: 0.3739  data_time: 0.0321  last_data_time: 0.0226   lr: 0.02  max_mem: 11505M
[32m[11/10 16:50:09 d2.utils.events]: [0m eta: 0:06:57  iter: 7519  total_loss: 0.3547  loss_cls: 0.08193  loss_box_reg: 0.2425  loss_rpn_cls: 0.004809  loss_rpn_loc: 0.008169    time: 0.4212  last_time: 0.3896  data_time: 0.0347  last_data_time: 0.0037   lr: 0.02  max_mem: 11505M
[32m[11/10 16:50:18 d2.utils.events]: [0m eta: 0:06:49  iter: 7539  total_loss: 0.3377  loss_cls: 0.08954  loss_box_reg: 0.2314  loss_rpn_cls: 0.005259  loss_rpn_loc: 0.006084    time: 0.4213  last_time: 0.4771  data_time: 0.0353  last_data_time: 0.0556   lr: 0.02  max_mem: 11505M
[32m[11/10 16:50:27 d2.utils.events]: [0m eta: 0:06:41  iter: 7559  total_loss: 0.3754  loss_cls: 0.1107  loss_box_reg: 0.2659  loss_rpn_cls: 0.004993  loss_rpn_loc: 0.00679    time: 0.4213  last_time: 0.4482  data_time: 0.0345  last_data_time: 0.0320   lr: 0.02  max_mem: 11505M
[32m[11/10 16:50:35 d2.utils.events]: [0m eta: 0:06:32  iter: 7579  total_loss: 0.4023  loss_cls: 0.1038  loss_box_reg: 0.2788  loss_rpn_cls: 0.002928  loss_rpn_loc: 0.006466    time: 0.4212  last_time: 0.4145  data_time: 0.0316  last_data_time: 0.0502   lr: 0.02  max_mem: 11505M
[32m[11/10 16:50:44 d2.utils.events]: [0m eta: 0:06:24  iter: 7599  total_loss: 0.3628  loss_cls: 0.09683  loss_box_reg: 0.2542  loss_rpn_cls: 0.006363  loss_rpn_loc: 0.007078    time: 0.4213  last_time: 0.3565  data_time: 0.0375  last_data_time: 0.0160   lr: 0.02  max_mem: 11505M
[32m[11/10 16:50:53 d2.utils.events]: [0m eta: 0:06:16  iter: 7619  total_loss: 0.3419  loss_cls: 0.09637  loss_box_reg: 0.2469  loss_rpn_cls: 0.005598  loss_rpn_loc: 0.007676    time: 0.4213  last_time: 0.4855  data_time: 0.0369  last_data_time: 0.0824   lr: 0.02  max_mem: 11505M
[32m[11/10 16:51:01 d2.utils.events]: [0m eta: 0:06:07  iter: 7639  total_loss: 0.371  loss_cls: 0.09083  loss_box_reg: 0.2736  loss_rpn_cls: 0.004889  loss_rpn_loc: 0.006186    time: 0.4213  last_time: 0.3631  data_time: 0.0345  last_data_time: 0.0113   lr: 0.02  max_mem: 11505M
[32m[11/10 16:51:10 d2.utils.events]: [0m eta: 0:05:59  iter: 7659  total_loss: 0.3605  loss_cls: 0.09081  loss_box_reg: 0.2671  loss_rpn_cls: 0.003799  loss_rpn_loc: 0.007115    time: 0.4213  last_time: 0.4311  data_time: 0.0329  last_data_time: 0.0284   lr: 0.02  max_mem: 11505M
[32m[11/10 16:51:18 d2.utils.events]: [0m eta: 0:05:50  iter: 7679  total_loss: 0.3512  loss_cls: 0.08015  loss_box_reg: 0.2524  loss_rpn_cls: 0.004164  loss_rpn_loc: 0.005369    time: 0.4214  last_time: 0.3998  data_time: 0.0300  last_data_time: 0.0339   lr: 0.02  max_mem: 11505M
[32m[11/10 16:51:26 d2.utils.events]: [0m eta: 0:05:41  iter: 7699  total_loss: 0.346  loss_cls: 0.07658  loss_box_reg: 0.2578  loss_rpn_cls: 0.005651  loss_rpn_loc: 0.007771    time: 0.4213  last_time: 0.4071  data_time: 0.0296  last_data_time: 0.0314   lr: 0.02  max_mem: 11505M
[32m[11/10 16:51:35 d2.utils.events]: [0m eta: 0:05:32  iter: 7719  total_loss: 0.3811  loss_cls: 0.0926  loss_box_reg: 0.2795  loss_rpn_cls: 0.005133  loss_rpn_loc: 0.006064    time: 0.4213  last_time: 0.4910  data_time: 0.0343  last_data_time: 0.0795   lr: 0.02  max_mem: 11505M
[32m[11/10 16:51:43 d2.utils.events]: [0m eta: 0:05:23  iter: 7739  total_loss: 0.3289  loss_cls: 0.08213  loss_box_reg: 0.2377  loss_rpn_cls: 0.004362  loss_rpn_loc: 0.006679    time: 0.4213  last_time: 0.3979  data_time: 0.0290  last_data_time: 0.0125   lr: 0.02  max_mem: 11505M
[32m[11/10 16:51:52 d2.utils.events]: [0m eta: 0:05:15  iter: 7759  total_loss: 0.3401  loss_cls: 0.08526  loss_box_reg: 0.2494  loss_rpn_cls: 0.005813  loss_rpn_loc: 0.006779    time: 0.4214  last_time: 0.4651  data_time: 0.0401  last_data_time: 0.0365   lr: 0.02  max_mem: 11505M
[32m[11/10 16:52:01 d2.utils.events]: [0m eta: 0:05:06  iter: 7779  total_loss: 0.3679  loss_cls: 0.08933  loss_box_reg: 0.2597  loss_rpn_cls: 0.004304  loss_rpn_loc: 0.007055    time: 0.4214  last_time: 0.4399  data_time: 0.0328  last_data_time: 0.0620   lr: 0.02  max_mem: 11505M
[32m[11/10 16:52:09 d2.utils.events]: [0m eta: 0:04:58  iter: 7799  total_loss: 0.4036  loss_cls: 0.1124  loss_box_reg: 0.2713  loss_rpn_cls: 0.004397  loss_rpn_loc: 0.007709    time: 0.4214  last_time: 0.4861  data_time: 0.0343  last_data_time: 0.0459   lr: 0.02  max_mem: 11505M
[32m[11/10 16:52:18 d2.utils.events]: [0m eta: 0:04:49  iter: 7819  total_loss: 0.3667  loss_cls: 0.08376  loss_box_reg: 0.2793  loss_rpn_cls: 0.004405  loss_rpn_loc: 0.006633    time: 0.4214  last_time: 0.4223  data_time: 0.0335  last_data_time: 0.0319   lr: 0.02  max_mem: 11505M
[32m[11/10 16:52:26 d2.utils.events]: [0m eta: 0:04:40  iter: 7839  total_loss: 0.3526  loss_cls: 0.09125  loss_box_reg: 0.2491  loss_rpn_cls: 0.003994  loss_rpn_loc: 0.005011    time: 0.4214  last_time: 0.3906  data_time: 0.0284  last_data_time: 0.0040   lr: 0.02  max_mem: 11505M
[32m[11/10 16:52:34 d2.utils.events]: [0m eta: 0:04:32  iter: 7859  total_loss: 0.3729  loss_cls: 0.09818  loss_box_reg: 0.2551  loss_rpn_cls: 0.004906  loss_rpn_loc: 0.006556    time: 0.4214  last_time: 0.4071  data_time: 0.0349  last_data_time: 0.0142   lr: 0.02  max_mem: 11505M
[32m[11/10 16:52:43 d2.utils.events]: [0m eta: 0:04:23  iter: 7879  total_loss: 0.3839  loss_cls: 0.09891  loss_box_reg: 0.2742  loss_rpn_cls: 0.004103  loss_rpn_loc: 0.007791    time: 0.4214  last_time: 0.4739  data_time: 0.0325  last_data_time: 0.0522   lr: 0.02  max_mem: 11505M
[32m[11/10 16:52:51 d2.utils.events]: [0m eta: 0:04:15  iter: 7899  total_loss: 0.4234  loss_cls: 0.1063  loss_box_reg: 0.3065  loss_rpn_cls: 0.007041  loss_rpn_loc: 0.00928    time: 0.4214  last_time: 0.4389  data_time: 0.0336  last_data_time: 0.0553   lr: 0.02  max_mem: 11505M
[32m[11/10 16:53:00 d2.utils.events]: [0m eta: 0:04:06  iter: 7919  total_loss: 0.3999  loss_cls: 0.1058  loss_box_reg: 0.2741  loss_rpn_cls: 0.003723  loss_rpn_loc: 0.006359    time: 0.4215  last_time: 0.3791  data_time: 0.0348  last_data_time: 0.0287   lr: 0.02  max_mem: 11505M
[32m[11/10 16:53:09 d2.utils.events]: [0m eta: 0:03:58  iter: 7939  total_loss: 0.3478  loss_cls: 0.098  loss_box_reg: 0.2448  loss_rpn_cls: 0.005422  loss_rpn_loc: 0.005942    time: 0.4215  last_time: 0.4585  data_time: 0.0297  last_data_time: 0.0231   lr: 0.02  max_mem: 11505M
[32m[11/10 16:53:17 d2.utils.events]: [0m eta: 0:03:49  iter: 7959  total_loss: 0.3665  loss_cls: 0.1057  loss_box_reg: 0.2665  loss_rpn_cls: 0.005225  loss_rpn_loc: 0.00629    time: 0.4215  last_time: 0.4102  data_time: 0.0344  last_data_time: 0.0346   lr: 0.02  max_mem: 11505M
[32m[11/10 16:53:26 d2.utils.events]: [0m eta: 0:03:40  iter: 7979  total_loss: 0.3691  loss_cls: 0.09408  loss_box_reg: 0.2627  loss_rpn_cls: 0.004434  loss_rpn_loc: 0.009112    time: 0.4215  last_time: 0.4899  data_time: 0.0375  last_data_time: 0.0589   lr: 0.02  max_mem: 11505M
[32m[11/10 16:53:34 d2.utils.events]: [0m eta: 0:03:32  iter: 7999  total_loss: 0.3931  loss_cls: 0.09986  loss_box_reg: 0.2806  loss_rpn_cls: 0.008138  loss_rpn_loc: 0.007835    time: 0.4215  last_time: 0.4160  data_time: 0.0291  last_data_time: 0.0209   lr: 0.02  max_mem: 11505M
[32m[11/10 16:53:43 d2.utils.events]: [0m eta: 0:03:23  iter: 8019  total_loss: 0.3648  loss_cls: 0.09761  loss_box_reg: 0.2403  loss_rpn_cls: 0.005607  loss_rpn_loc: 0.007263    time: 0.4215  last_time: 0.3789  data_time: 0.0381  last_data_time: 0.0318   lr: 0.02  max_mem: 11505M
[32m[11/10 16:53:52 d2.utils.events]: [0m eta: 0:03:15  iter: 8039  total_loss: 0.3392  loss_cls: 0.09124  loss_box_reg: 0.2349  loss_rpn_cls: 0.00501  loss_rpn_loc: 0.008475    time: 0.4216  last_time: 0.4205  data_time: 0.0350  last_data_time: 0.0501   lr: 0.02  max_mem: 11505M
[32m[11/10 16:54:00 d2.utils.events]: [0m eta: 0:03:06  iter: 8059  total_loss: 0.3608  loss_cls: 0.0862  loss_box_reg: 0.2607  loss_rpn_cls: 0.003725  loss_rpn_loc: 0.00521    time: 0.4216  last_time: 0.3911  data_time: 0.0342  last_data_time: 0.0357   lr: 0.02  max_mem: 11505M
[32m[11/10 16:54:09 d2.utils.events]: [0m eta: 0:02:58  iter: 8079  total_loss: 0.3208  loss_cls: 0.08234  loss_box_reg: 0.2223  loss_rpn_cls: 0.003837  loss_rpn_loc: 0.008901    time: 0.4216  last_time: 0.4322  data_time: 0.0361  last_data_time: 0.0481   lr: 0.02  max_mem: 11505M
[32m[11/10 16:54:18 d2.utils.events]: [0m eta: 0:02:49  iter: 8099  total_loss: 0.3314  loss_cls: 0.08929  loss_box_reg: 0.2259  loss_rpn_cls: 0.003908  loss_rpn_loc: 0.005413    time: 0.4216  last_time: 0.4086  data_time: 0.0369  last_data_time: 0.0417   lr: 0.02  max_mem: 11505M
[32m[11/10 16:54:26 d2.utils.events]: [0m eta: 0:02:41  iter: 8119  total_loss: 0.3626  loss_cls: 0.09359  loss_box_reg: 0.2365  loss_rpn_cls: 0.006106  loss_rpn_loc: 0.008272    time: 0.4216  last_time: 0.4143  data_time: 0.0309  last_data_time: 0.0528   lr: 0.02  max_mem: 11505M
[32m[11/10 16:54:34 d2.utils.events]: [0m eta: 0:02:32  iter: 8139  total_loss: 0.4308  loss_cls: 0.121  loss_box_reg: 0.287  loss_rpn_cls: 0.006022  loss_rpn_loc: 0.006785    time: 0.4217  last_time: 0.4473  data_time: 0.0332  last_data_time: 0.0274   lr: 0.02  max_mem: 11505M
[32m[11/10 16:54:43 d2.utils.events]: [0m eta: 0:02:24  iter: 8159  total_loss: 0.3671  loss_cls: 0.09468  loss_box_reg: 0.2595  loss_rpn_cls: 0.004243  loss_rpn_loc: 0.007005    time: 0.4217  last_time: 0.4416  data_time: 0.0300  last_data_time: 0.0380   lr: 0.02  max_mem: 11505M
[32m[11/10 16:54:52 d2.utils.events]: [0m eta: 0:02:16  iter: 8179  total_loss: 0.3616  loss_cls: 0.09019  loss_box_reg: 0.2533  loss_rpn_cls: 0.005046  loss_rpn_loc: 0.007045    time: 0.4217  last_time: 0.4170  data_time: 0.0358  last_data_time: 0.0437   lr: 0.02  max_mem: 11505M
[32m[11/10 16:55:00 d2.utils.events]: [0m eta: 0:02:07  iter: 8199  total_loss: 0.361  loss_cls: 0.08865  loss_box_reg: 0.257  loss_rpn_cls: 0.005928  loss_rpn_loc: 0.009144    time: 0.4217  last_time: 0.3835  data_time: 0.0333  last_data_time: 0.0380   lr: 0.02  max_mem: 11505M
[32m[11/10 16:55:08 d2.utils.events]: [0m eta: 0:01:58  iter: 8219  total_loss: 0.3394  loss_cls: 0.07219  loss_box_reg: 0.2411  loss_rpn_cls: 0.007423  loss_rpn_loc: 0.007294    time: 0.4216  last_time: 0.4208  data_time: 0.0285  last_data_time: 0.0232   lr: 0.02  max_mem: 11505M
[32m[11/10 16:55:17 d2.utils.events]: [0m eta: 0:01:50  iter: 8239  total_loss: 0.3522  loss_cls: 0.09761  loss_box_reg: 0.2496  loss_rpn_cls: 0.003249  loss_rpn_loc: 0.007115    time: 0.4217  last_time: 0.4884  data_time: 0.0363  last_data_time: 0.0803   lr: 0.02  max_mem: 11505M
[32m[11/10 16:55:26 d2.utils.events]: [0m eta: 0:01:42  iter: 8259  total_loss: 0.3644  loss_cls: 0.09431  loss_box_reg: 0.2411  loss_rpn_cls: 0.003928  loss_rpn_loc: 0.005691    time: 0.4217  last_time: 0.4145  data_time: 0.0345  last_data_time: 0.0087   lr: 0.02  max_mem: 11505M
[32m[11/10 16:55:34 d2.utils.events]: [0m eta: 0:01:33  iter: 8279  total_loss: 0.3522  loss_cls: 0.08903  loss_box_reg: 0.2368  loss_rpn_cls: 0.004362  loss_rpn_loc: 0.008274    time: 0.4217  last_time: 0.3986  data_time: 0.0293  last_data_time: 0.0192   lr: 0.02  max_mem: 11505M
[32m[11/10 16:55:43 d2.utils.events]: [0m eta: 0:01:24  iter: 8299  total_loss: 0.3496  loss_cls: 0.08809  loss_box_reg: 0.2385  loss_rpn_cls: 0.004014  loss_rpn_loc: 0.006876    time: 0.4217  last_time: 0.3928  data_time: 0.0318  last_data_time: 0.0112   lr: 0.02  max_mem: 11505M
[32m[11/10 16:55:51 d2.utils.events]: [0m eta: 0:01:16  iter: 8319  total_loss: 0.3776  loss_cls: 0.1008  loss_box_reg: 0.2516  loss_rpn_cls: 0.006895  loss_rpn_loc: 0.006704    time: 0.4217  last_time: 0.3776  data_time: 0.0372  last_data_time: 0.0328   lr: 0.02  max_mem: 11505M
[32m[11/10 16:55:59 d2.utils.events]: [0m eta: 0:01:07  iter: 8339  total_loss: 0.3474  loss_cls: 0.08413  loss_box_reg: 0.2391  loss_rpn_cls: 0.004265  loss_rpn_loc: 0.005877    time: 0.4217  last_time: 0.4083  data_time: 0.0315  last_data_time: 0.0114   lr: 0.02  max_mem: 11505M
[32m[11/10 16:56:08 d2.utils.events]: [0m eta: 0:00:59  iter: 8359  total_loss: 0.3879  loss_cls: 0.1048  loss_box_reg: 0.2701  loss_rpn_cls: 0.005035  loss_rpn_loc: 0.006524    time: 0.4217  last_time: 0.3932  data_time: 0.0298  last_data_time: 0.0482   lr: 0.02  max_mem: 11505M
[32m[11/10 16:56:16 d2.utils.events]: [0m eta: 0:00:50  iter: 8379  total_loss: 0.3342  loss_cls: 0.08338  loss_box_reg: 0.2442  loss_rpn_cls: 0.003365  loss_rpn_loc: 0.006024    time: 0.4217  last_time: 0.4027  data_time: 0.0298  last_data_time: 0.0114   lr: 0.02  max_mem: 11505M
[32m[11/10 16:56:25 d2.utils.events]: [0m eta: 0:00:42  iter: 8399  total_loss: 0.3833  loss_cls: 0.1068  loss_box_reg: 0.2629  loss_rpn_cls: 0.005805  loss_rpn_loc: 0.007089    time: 0.4217  last_time: 0.4787  data_time: 0.0421  last_data_time: 0.0705   lr: 0.02  max_mem: 11505M
[32m[11/10 16:56:33 d2.utils.events]: [0m eta: 0:00:33  iter: 8419  total_loss: 0.3368  loss_cls: 0.07785  loss_box_reg: 0.2543  loss_rpn_cls: 0.004667  loss_rpn_loc: 0.006863    time: 0.4218  last_time: 0.3703  data_time: 0.0349  last_data_time: 0.0112   lr: 0.02  max_mem: 11505M
[32m[11/10 16:56:42 d2.utils.events]: [0m eta: 0:00:25  iter: 8439  total_loss: 0.3406  loss_cls: 0.09292  loss_box_reg: 0.2365  loss_rpn_cls: 0.005627  loss_rpn_loc: 0.006787    time: 0.4217  last_time: 0.4532  data_time: 0.0317  last_data_time: 0.0554   lr: 0.02  max_mem: 11505M
[32m[11/10 16:56:50 d2.utils.events]: [0m eta: 0:00:16  iter: 8459  total_loss: 0.3697  loss_cls: 0.09753  loss_box_reg: 0.2718  loss_rpn_cls: 0.006063  loss_rpn_loc: 0.01033    time: 0.4217  last_time: 0.4386  data_time: 0.0289  last_data_time: 0.0164   lr: 0.02  max_mem: 11505M
[32m[11/10 16:56:58 d2.utils.events]: [0m eta: 0:00:08  iter: 8479  total_loss: 0.3653  loss_cls: 0.09351  loss_box_reg: 0.2538  loss_rpn_cls: 0.004838  loss_rpn_loc: 0.006833    time: 0.4217  last_time: 0.3906  data_time: 0.0338  last_data_time: 0.0076   lr: 0.02  max_mem: 11505M
[32m[11/10 16:57:08 d2.utils.events]: [0m eta: 0:00:00  iter: 8499  total_loss: 0.404  loss_cls: 0.1003  loss_box_reg: 0.276  loss_rpn_cls: 0.004273  loss_rpn_loc: 0.006899    time: 0.4217  last_time: 0.4375  data_time: 0.0318  last_data_time: 0.0213   lr: 0.02  max_mem: 11505M
[32m[11/10 16:57:08 d2.engine.hooks]: [0mOverall training speed: 8498 iterations in 0:59:43 (0.4217 s / it)
[32m[11/10 16:57:08 d2.engine.hooks]: [0mTotal training time: 0:59:50 (0:00:06 on hooks)
[32m[11/10 16:57:08 d2.data.datasets.coco]: [0mLoaded 177 images in COCO format from ../datasets/coco/annotations/dog_instances_val2017.json
[32m[11/10 16:57:08 d2.data.build]: [0mDistribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    dog     | 218          |
|            |              |[0m
[32m[11/10 16:57:08 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[32m[11/10 16:57:08 d2.data.common]: [0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[32m[11/10 16:57:08 d2.data.common]: [0mSerializing 177 elements to byte tensors and concatenating them all ...
[32m[11/10 16:57:08 d2.data.common]: [0mSerialized dataset takes 0.24 MiB
[5m[31mWARNING[0m [32m[11/10 16:57:08 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
>>> trainer.train()model.to(cfg.MODEL.DEVICE)[C[C[C[C[C[C[C[C[C[C[C[Ctrain()[K[C[C[C[C[Ktrainer.model.eval()
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=2, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
    )
  )
)
>>> evaluator = COCOEvaluator("coco_val_subset", ("bbox",), False, output_dir="./knize/output/subset/trained")[C[1P/trained")[1P/trained")[1P/trained")[1P/trained")[1P/trained")[1P/trained")d/trained")o/trained")g/trained")[C[C[C[C[C[C[C[C[C[C[1P[1P[1P[1P[1P[1P[1@d[1@o[1@g
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'COCOEvaluator' is not defined
>>> evaluator = COCOEvaluator("coco_val_dog", ("bbox",), False, output_dir="./knize/output/dog/trained")[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Kfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset[K[K[K[K[K[K[K[K[K[K[K[K[C[C[C[C[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P inferenc[C[C[C[C[1Pinferenc[C[C[C[C[1Pnferenc[C[C[C[C[1Pferenc[C[C[C[C[1Perenc[1Prenc[1Penc[1Pnc[1Pc[Kfrom detectron2.evaluation import COCOEvaluator, inference_on_datasetfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset
[1]+  Stopped                 CUDA_VISIBLE_DEVICES=2 python
jknize@aiscalar:~/main/repo/CSC578$ CUDA_VISIBLE_DEVICES-2[K=2[K[K[K=2 python
Python 3.8.10 (default, Nov 22 2023, 10:22:35) 
[GCC 9.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import os
>>> os.chdir('/home/jknize/main/repo/CSC578/detectron2')
>>> from detectron2.data.datasets import register_coco_instances
>>> from detectron2.data import DatasetCatalog, MetadataCatalog
>>> from detectron2.config import get_cfg
>>> from detectron2.engine import DefaultTrainer
>>> import torch
>>> from detectron2.evaluation import COCOEvaluator, inference_on_dataset
>>> from detectron2.data import build_detection_test_loader
>>> register_coco_instances("coco_train_subset", {}, "../datasets/coco/annotations/filtered_instances_train2017_2.json", "../datasets/coco/train2017_subset")
>>> register_coco_instances("coco_train_subset", {}, "../datasets/coco/annotations/filtered_instances_train2017_2.json", "../datasets/coco/train2017_subset")M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[K
[KM[C[C[C[Cregister_coco_instances("coco_train_dog", {}, "../datasets/coco/annotations/dog_instances_train2017.json", "../datasets/coco/train2017_dog")
>>> register_coco_instances("coco_val_dog", {}, "../datasets/coco/annotations/dog_instances_val2017.json", "../datasets/coco/val2017_dog")
>>> my_dataset_metadata = MetadataCatalog.get("coco_train_dog")
>>> my_dataset_metadata.thing_classes = ["dog"]
>>> cfg = get_cfg()
>>> cfg.merge_from_file("configs/COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml") #ImageNet pre-trained
>>> cfg.OUTPUT_DIR = "knize/output/dog"
>>> cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5
>>> cfg.DATASETS.TRAIN = ("coco_train_dog",)
>>> cfg.DATASETS.TEST = ("coco_val_dog",)
>>> cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1cfg.MODEL.WEIGHTS = "knize/output/subset/model_final.pth"[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K
>>> cfg.MODEL.WEIGHTS = "knize/output/subset/model_final.pth"[C[1P/model_final.pth"[1P/model_final.pth"[1P/model_final.pth"[1P/model_final.pth"[1P/model_final.pth"[1P/model_final.pth"d/model_final.pth"o/model_final.pth"g/model_final.pth"
>>> cfg.MODEL.DEVICE = 'cuda'
>>> cfg.SOLVER.IMS_PER_BATCH = 8
>>> trainer = DefaultTrainer(cfg)
[32m[11/10 17:47:42 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=2, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
    )
  )
)
[32m[11/10 17:47:42 d2.data.datasets.coco]: [0mLoaded 4385 images in COCO format from ../datasets/coco/annotations/dog_instances_train2017.json
[32m[11/10 17:47:42 d2.data.build]: [0mRemoved 0 images with no usable annotations. 4385 images left.
[32m[11/10 17:47:42 d2.data.build]: [0mDistribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    dog     | 5500         |
|            |              |[0m
[32m[11/10 17:47:42 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[11/10 17:47:42 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[11/10 17:47:42 d2.data.common]: [0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[32m[11/10 17:47:42 d2.data.common]: [0mSerializing 4385 elements to byte tensors and concatenating them all ...
[32m[11/10 17:47:42 d2.data.common]: [0mSerialized dataset takes 6.35 MiB
[32m[11/10 17:47:42 d2.data.build]: [0mMaking batched data loader with batch_size=8
>>> trainer.resume_or_load(resume=False)
[32m[11/10 17:47:45 d2.checkpoint.detection_checkpoint]: [0m[DetectionCheckpointer] Loading from knize/output/dog/model_final.pth ...
>>> trainer.model.to(cfg.MODEL.DEVICE)
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=2, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
    )
  )
)
>>> trainer.model.eval()
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=2, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
    )
  )
)
>>> evaluator = COCOEvaluator("coco_val_dog", ("bbox",), False, output_dir="./knize/output/subset/trained")
[32m[11/10 17:47:58 d2.evaluation.coco_evaluation]: [0mFast COCO eval is not built. Falling back to official COCO eval.
>>> val_loader = build_detection_test_loader(cfg, "coco_val_dog")
[32m[11/10 17:48:03 d2.data.datasets.coco]: [0mLoaded 177 images in COCO format from ../datasets/coco/annotations/dog_instances_val2017.json
[32m[11/10 17:48:03 d2.data.build]: [0mDistribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    dog     | 218          |
|            |              |[0m
[32m[11/10 17:48:03 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[32m[11/10 17:48:03 d2.data.common]: [0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[32m[11/10 17:48:03 d2.data.common]: [0mSerializing 177 elements to byte tensors and concatenating them all ...
[32m[11/10 17:48:03 d2.data.common]: [0mSerialized dataset takes 0.24 MiB
>>> print(inference_on_dataset(trainer.model, val_loader, evaluator))
[32m[11/10 17:48:09 d2.evaluation.evaluator]: [0mStart inference on 177 batches
/home/jknize/.local/lib/python3.8/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[32m[11/10 17:48:10 d2.evaluation.evaluator]: [0mInference done 11/177. Dataloading: 0.0007 s/iter. Inference: 0.0778 s/iter. Eval: 0.0002 s/iter. Total: 0.0787 s/iter. ETA=0:00:13
[32m[11/10 17:48:16 d2.evaluation.evaluator]: [0mInference done 89/177. Dataloading: 0.0014 s/iter. Inference: 0.0654 s/iter. Eval: 0.0002 s/iter. Total: 0.0671 s/iter. ETA=0:00:05
[32m[11/10 17:48:21 d2.evaluation.evaluator]: [0mInference done 173/177. Dataloading: 0.0015 s/iter. Inference: 0.0616 s/iter. Eval: 0.0002 s/iter. Total: 0.0633 s/iter. ETA=0:00:00
[32m[11/10 17:48:21 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:10.900087 (0.063373 s / iter per device, on 1 devices)
[32m[11/10 17:48:21 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:10 (0.061241 s / iter per device, on 1 devices)
[32m[11/10 17:48:21 d2.evaluation.coco_evaluation]: [0mPreparing results for COCO format ...
[32m[11/10 17:48:21 d2.evaluation.coco_evaluation]: [0mSaving results to ./knize/output/subset/trained/coco_instances_results.json
[32m[11/10 17:48:21 d2.evaluation.coco_evaluation]: [0mEvaluating predictions with official COCO API...
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.15s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.494
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.776
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.556
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.319
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.538
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.478
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.584
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.584
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.326
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.611
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.607
[32m[11/10 17:48:21 d2.evaluation.coco_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 49.353 | 77.553 | 55.575 | 31.879 | 53.785 | 50.352 |
OrderedDict([('bbox', {'AP': 49.35341477990296, 'AP50': 77.55285876987526, 'AP75': 55.57477233115292, 'APs': 31.87867358164388, 'APm': 53.78511990360339, 'APl': 50.35232295258805})])
>>> exit(0[K)
jknize@aiscalar:~/main/repo/CSC578$ exit
exit
There are stopped jobs.
jknize@aiscalar:~/main/repo/CSC578$ exit
exit

Script done on 2024-11-10 17:50:24-06:00 [COMMAND_EXIT_CODE="1"]
